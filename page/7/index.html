
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>小明明s à domicile</title>
  <meta name="author" content="Dongweiming">

  
  <meta name="description" content="douban dongweiming site">
  <meta name="keywords" content="python, ipython, emacs, github, dongweiming, django, flask, bottle, jinja2, requests, douban, httpie, jedi, mako, plim, react, develop, lisp, ruby, web development, sed, awk, linux, 运维, 运维开发, sentry, tonrado, scrapy, fabric, celery">
             

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://dongweiming.github.com/blog/page/7">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <script type="text/javascript" src="/javascripts/api.js"></script>
  <script type="text/javascript" src="/javascripts/wordcumulus.js"></script>
  <script type="text/javascript" src="/javascripts/swfobject.js"></script>
  <script type="text/javascript" src="/javascripts/tagcumulus.js"></script>
  <link href="/atom.xml" rel="alternate" title="小明明s à domicile" type="application/atom+xml">
  <script type="text/javascript" src="/javascripts/jimdoclockzip.js"></script>
<script type="text/javascript" src="/javascripts/sh_python.min.js"></script>
<script type="text/javascript" src="/javascripts/sh_bash.min.js"></script>
<script type="text/javascript" src="/javascripts/sh_main.min.js"></script>
<link href="/stylesheets/sh_ide-anjuta.css" rel="stylesheet" type="text/css">

  
<script id="search-results-template" type="text/x-handlebars-template">
  {{#entries}}
    <article>
        <h3>
            <small><time datetime="{{date}}" pubdate>{{date}}</time></small>
            <a href="{{url}}">{{title}}</a>
            <p>tagged: {{ tags }} | category: <a href="/categories/{{category }}">{{category}}</a></p>
        </h3>
    </article>
  {{/entries}}
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-20495125-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body    onload="sh_highlightDocument('', '.js');">
<a href="http://github.com/dongweiming/">
<img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Follower me on GitHub">
</a>
  <nav role="navigation"><div class="navbar">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">小明明s à domicile</a>

      <div class="nav-collapse">
          <ul class="nav">
    <li><a href="/">博客主页</a></li>
    <li><a href="/blog/archives">文章列表</a></li>
    <li><a href="/aboutsite">关于本站</a></li>
    <li><a href="/projects">我的项目</a></li>
    <li><a href="http://dongweiming.github.io/sed_and_awk">sed_and_awk</a></li>
    <li><a href="http://dongweiming.github.io/Expert-Python">Expert-Python</a></li>
    <li><a href="/aboutme">关于我</a></li>
</ul>

          <ul class="nav pull-right" data-subscription="rss">
              <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
          </ul>

        

          
            <form action="/search" method="get" class="pull-right navbar-search">
    <fieldset role ="search">
        <input type="text" id="search-query" name="q" placeholder="Search" autocomplete="off" class="search" />
    </fieldset>
</form>

          
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
      <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/gevent-twisted-duo-xian-cheng-shui-geng-kuai/">Gevent-twisted-多线程谁更快?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-01-11T13:54:00+08:00" pubdate data-updated="true">Jan 11<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>标题有点唬人，以前了解过研究gevent，twisted，scrapy（基于twisted）。最近有个想法：这些东西比如做爬虫，谁的效率更好呢？
我就写了以下程序（附件）测试然后用timeit（跑3次，每次10遍，时间有限）看效果</p>

<h4 id="section-1">原理：</h4>

<ol>
  <li>为了防止远程网络的问题，从一个网站爬下网页代码（html），页面下载本地放在了我的本机（gentoo+apache）</li>
  <li>然后爬虫去分析这些页面上面的链接（开始是主页），再挖掘其他页面，抓取页面关键字（我这里就是个‘py’）
程序打包<a href="http://www.dongwm.com/Crawler.tar.bz2">Crawler.tar.bz2</a></li>
</ol>

<p>先看代码树：</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
dongwm@localhost ~ $ tree Crawler/
Crawler/
├── common_Crawler.py  #标准爬虫，里面只是多线程编程，抓取分析类在common.py
├── common.py  #共用函数，里面只是抓取页面分析页面关键字
├── common.pyc #你懂得
├── Crawler #scrapy和django框架差不多的用法
│   ├── __init__.py
│   ├── __init__.pyc
│   ├── items.py #不需要利用，默认
│   ├── pipelines.py
│   ├── settings.py
│   ├── settings.pyc
│   └── spiders #抓取脚本文件夹
│       ├── __init__.py
│       ├── __init__.pyc
│       ├── spiders.py #我做的分析页面，这个和多线程/gevent调用的抓取分析类不同，我使用了内置方法（大家可以修改共用函数改成scrapy的方式，这样三种效果就更准确了）
│       └── spiders.pyc
├── gevent_Crawler.py #gevent版本爬虫，效果和标准版一样，抓取分析类也是common.py 保证其他环节相同，只是一个多线程，一个用协程
├── scrapy.cfg
└── scrapy_Crawler.py #因为scrapy使用是命令行，我用subproess封装了命令，然后使用timeit计算效果

2 directories, 16 files
</pre></figure></notextile></div></notextile></div>

<h4 id="section-2">实验前准备：</h4>
<p>停掉我本机使用的耗费资源的进程 firefox，vmware，compiz等，直到负载保持一个相对拨波动平衡</p>

<h4 id="section-3">测试程序：</h4>

<ol>
  <li>common.py </li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
#!/usr/bin/python
#coding=utf-8

# Version 1 by Dongwm 2013/01/10
# 脚本作用：多线程抓取
# 方式： lxml + xpath + requests

import requests
from  cStringIO import StringIO
from lxml import etree

class Crawler(object):

    def __init__(self, app):
        self.deep = 2  #指定网页的抓取深度        
        self.url = '' #指定网站地址
        self.key = 'by' #搜索这个词
        self.tp = app #连接池回调实例
        self.visitedUrl = [] #抓取的网页放入列表,防止重复抓取

    def _hasCrawler(self, url): 
        '''判断是否已经抓取过这个页面'''
        return (True if url in self.visitedUrl else False)
     
    def getPageSource(self, url, key, deep): 
        ''' 抓取页面,分析,入库.
        '''
        if self._hasCrawler(url): #发现重复直接return
            return 
        else:
            self.visitedUrl.append(url) #发现新地址假如到这个列
        r = requests.get('http://localhost/%s' % url)
        encoding = r.encoding #判断页面的编码
        result = r.text.encode('utf-8').decode(encoding)
	    #f = StringIO(r.text.encode('utf-8'))
        try:  
            self._xpath(url, result, ['a'], unicode(key, 'utf8'), deep) #分析页面中的连接地址,以及它的内容
            self._xpath(url, result, ['title', 'p', 'li', 'div'], unicode(key, "utf8"), deep) #分析这几个标签的内容
        except TypeError: #对编码类型异常处理,有些深度页面和主页的编码不同
            self._xpath(url, result, ['a'], key, deep)
            self._xpath(url, result, ['title', 'p', 'li', 'div'], key, deep)
        return True

    def _xpath(self, weburl, data, xpath, key, deep):
        page = etree.HTML(data)
        for i in xpath:
            hrefs = page.xpath(u"//%s" % i) #根据xpath标签
            if deep &gt;1:
                for href in hrefs:
                    url = href.attrib.get('href','')
                    if not url.startswith('java') and not url.startswith('#') and not \
                        url.startswith('mailto') and url.endswith('html'):  #过滤javascript和发送邮件的链接
                            self.tp.add_job(self.getPageSource,url, key, deep-1) #递归调用,直到符合的深
            for href in hrefs:
                value = href.text  #抓取相应标签的内容
                if value:
                    m = re.compile(r'.*%s.*' % key).match(value) #根据key匹配相应内容

    def work(self):
        self.tp.add_job(self.getPageSource, self.url, self.key, self.deep)
        self.tp.wait_for_complete() #等待线程池完成
</pre></figure></notextile></div></notextile></div>

<ol>
  <li>common_Crawler.py</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
#!/usr/bin/python
#coding=utf-8

# Version 1 by Dongwm 2013/01/10
# 脚本作用：多线程



import time
import threading
import Queue
from common import Crawler

#lock = threading.Lock()   #设置线程锁


class MyThread(threading.Thread):

    def __init__(self, workQueue, timeout=1, **kwargs):
        threading.Thread.__init__(self, kwargs=kwargs)
        self.timeout = timeout #线程在结束前等待任务队列多长时间
        self.setDaemon(True)  #设置deamon,表示主线程死掉,子线程不跟随死掉
        self.workQueue = workQueue
        self.start() #初始化直接启动线程

    def run(self):
        '''重载run方法'''
        while True:
            try:
                #lock.acquire() #线程安全上锁 PS:queue 实现就是线程安全的，没有必要上锁 ,否者可以put/get_nowait
                callable, args = self.workQueue.get(timeout=self.timeout) #从工作队列中获取一个任务
                res = callable(*args)  #执行的任务
                #lock.release()  #执行完,释放锁 
            except Queue.Empty: #任务队列空的时候结束此线程
                break
            except Exception, e:
                return -1


class ThreadPool(object):

    def __init__(self, num_of_threads):
         self.workQueue = Queue.Queue()
         self.threads = []
         self.__createThreadPool(num_of_threads)
 
    def __createThreadPool(self, num_of_threads):
        for i in range(num_of_threads):
             thread = MyThread(self.workQueue)
             self.threads.append(thread)

    def wait_for_complete(self):
        '''等待所有线程完成'''
        while len(self.threads):
            thread = self.threads.pop()
            if thread.isAlive():  #判断线程是否还存活来决定是否调用join
                thread.join()
     
    def add_job( self, callable, *args):
        '''增加任务,放到队列里面'''
        self.workQueue.put((callable, args))
def main():

    tp = ThreadPool(10) 
    crawler = Crawler(tp)
    crawler.work()

if __name__ == '__main__':

    import timeit
    t = timeit.Timer("main()") 
    t.repeat(3, 10)
</pre></figure></notextile></div></notextile></div>

<ol>
  <li>gevent_Crawler.py</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
#!/usr/bin/python
#coding=utf-8

# Version 1 by Dongwm 2013/01/10
# 脚本作用：gevent

import gevent.monkey
gevent.monkey.patch_all()
from gevent.queue import Empty, Queue
import gevent
from common import Crawler

class GeventLine(object):

    def __init__(self, workQueue, timeout=1, **kwargs):
        self.timeout = timeout #线程在结束前等待任务队列多长时间
        self.workQueue = workQueue

    def run(self):
        '''重载run方法'''
        while True:
            try:
                callable, args = self.workQueue.get(timeout=self.timeout) #从工作队列中获取一个任务
                res = callable(*args)  #执行的任务
                print res
            except Empty:
                break
            except Exception, e:
            	print e
                return -1

class GeventPool(object):

	def __init__(self, num_of_threads):
	         self.workQueue = Queue()
	         self.threads = []
	         self.__createThreadPool(num_of_threads)
	 
	def __createThreadPool(self, num_of_threads):
	    for i in range(num_of_threads):
	         thread = GeventLine(self.workQueue)
	         self.threads.append(gevent.spawn(thread.run))


	def wait_for_complete(self):
	    '''等待所有线程完成'''

	    while len(self.threads):
	        thread = self.threads.pop()
	        thread.join()
	    gevent.shutdown()
	 
	def add_job( self, callable, *args):
	    '''增加任务,放到队列里面'''
	    self.workQueue.put((callable, args))

def main():
	tp = GeventPool(10) 
	crawler = Crawler(tp)
	crawler.work()

if __name__ == '__main__':

    import timeit
    t = timeit.Timer("main()") 
    t.repeat(3, 10)

</pre></figure></notextile></div></notextile></div>

<ol>
  <li>Crawler/spiders/spiders.py</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import HtmlXPathSelector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.item import Item

class MySpider(CrawlSpider):
    name = 'localhost'
    allowed_domains = ['localhost']
    start_urls = ['http://localhost']
    rules = ( 
        Rule(SgmlLinkExtractor(allow=(r'http://localhost/.*')), callback="parse_item"),  
    )  
    def parse_item(self, response):
        hxs = HtmlXPathSelector(response)
        hxs.select('//*[@*]/text()').re(r'py')  #实现了common.py里面的抓取和分析，但是common.py是抓取五种标签，分2次抓取，这里是抓取所有标签，不够严禁

</pre></figure></notextile></div></notextile></div>

<ol>
  <li>scrapy_Crawler.py #时间有限，没有研究模块调用，也不够严禁</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">

#!/usr/bin/python
#coding=utf-8

# Version 1 by Dongwm 2013/01/10
# 脚本作用：scrapy

from subprocess import call

def main():
	call('scrapy crawl localhost --nolog', shell=True)

if __name__ == '__main__':

    import timeit
    t = timeit.Timer("main()") 
    t.repeat(3, 10)
</pre></figure></notextile></div></notextile></div>

<h4 id="section-4">实验过程</h4>

<h5 id="section-5">1. 同时启动三个终端，一起跑（手点回车，肯定有点延迟）</h5>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
dongwm@localhost ~/Crawler $ python scrapy_Crawler.py
10000000 loops, best of 3: 0.024 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0223 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0223 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0223 usec per loop
10000000 loops, best of 3: 0.0222 usec per loop
10000000 loops, best of 3: 0.0223 usec per loop #他是最快跑完的，非常快～～  数据很稳定

dongwm@localhost ~/Crawler $ python gevent_Crawler.py
100000000 loops, best of 3: 0.0134 usec per loop
100000000 loops, best of 3: 0.0131 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0134 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0133 usec per loop
100000000 loops, best of 3: 0.0133 usec per loop
100000000 loops, best of 3: 0.0133 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0133 usec per loop
100000000 loops, best of 3: 0.0132 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0123 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0123 usec per loop  #跑得很慢，不知道是不是timeit的原因(或者调用的优先级太低，抢资源能力不行)，很奇怪，但是它的数据最快，数据稳定在0.0123-0.0133


dongwm@localhost ~/Crawler $ python common_Crawler.py
100000000 loops, best of 3: 0.0274 usec per loop
10000000 loops, best of 3: 0.0245 usec per loop
10000000 loops, best of 3: 0.0252 usec per loop
10000000 loops, best of 3: 0.0239 usec per loop
10000000 loops, best of 3: 0.025 usec per loop
10000000 loops, best of 3: 0.0273 usec per loop
10000000 loops, best of 3: 0.0255 usec per loop
10000000 loops, best of 3: 0.0261 usec per loop
10000000 loops, best of 3: 0.0275 usec per loop
10000000 loops, best of 3: 0.0261 usec per loop
10000000 loops, best of 3: 0.0257 usec per loop
10000000 loops, best of 3: 0.0273 usec per loop
10000000 loops, best of 3: 0.0241 usec per loop
10000000 loops, best of 3: 0.0257 usec per loop
10000000 loops, best of 3: 0.0275 usec per loop
10000000 loops, best of 3: 0.0241 usec per loop
10000000 loops, best of 3: 0.0259 usec per loop
10000000 loops, best of 3: 0.0251 usec per loop
10000000 loops, best of 3: 0.0193 usec per loop
10000000 loops, best of 3: 0.0176 usec per loop
100000000 loops, best of 3: 0.0199 usec per loop
100000000 loops, best of 3: 0.0167 usec per loop
100000000 loops, best of 3: 0.018 usec per loop
10000000 loops, best of 3: 0.0179 usec per loop
100000000 loops, best of 3: 0.0173 usec per loop
100000000 loops, best of 3: 0.0172 usec per loop
100000000 loops, best of 3: 0.018 usec per loop
100000000 loops, best of 3: 0.0162 usec per loop
100000000 loops, best of 3: 0.0179 usec per loop
100000000 loops, best of 3: 0.0171 usec per loop  #第二跑得快，但是还是数据不稳定，时间在0.017-0.026之间
</pre></figure></notextile></div></notextile></div>
<p>#####2. 挨个启动，待负载保持一个相对拨波动平衡 在换另一个</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
dongwm@localhost ~/Crawler $ python scrapy_Crawler.py
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0122 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0123 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0123 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0122 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0123 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0122 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0122 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop   #数据很稳定，在0.0122-0.0126之间 机器负载在1.3左右,最高超过了1.4（闲暇0.6左右）
</pre></figure></notextile></div></notextile></div>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
dongwm@localhost ~/Crawler $ python gevent_Crawler.py
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0126 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop
100000000 loops, best of 3: 0.0125 usec per loop
100000000 loops, best of 3: 0.0124 usec per loop  #数据很稳定，在0.0124-0.0126之间 机器负载在1.2左右（闲暇0.6左右）
</pre></figure></notextile></div></notextile></div>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
dongwm@localhost ~/Crawler $ python common_Crawler.py
10000000 loops, best of 3: 0.0135 usec per loop
100000000 loops, best of 3: 0.0185 usec per loop
10000000 loops, best of 3: 0.0174 usec per loop
100000000 loops, best of 3: 0.019 usec per loop
10000000 loops, best of 3: 0.016 usec per loop
10000000 loops, best of 3: 0.0181 usec per loop
10000000 loops, best of 3: 0.0146 usec per loop
100000000 loops, best of 3: 0.0192 usec per loop
10000000 loops, best of 3: 0.0165 usec per loop
10000000 loops, best of 3: 0.0176 usec per loop
10000000 loops, best of 3: 0.0177 usec per loop
10000000 loops, best of 3: 0.0182 usec per loop
100000000 loops, best of 3: 0.0195 usec per loop
10000000 loops, best of 3: 0.0163 usec per loop
10000000 loops, best of 3: 0.0161 usec per loop
100000000 loops, best of 3: 0.0191 usec per loop
100000000 loops, best of 3: 0.0193 usec per loop
10000000 loops, best of 3: 0.0147 usec per loop
100000000 loops, best of 3: 0.0197 usec per loop
10000000 loops, best of 3: 0.0178 usec per loop
10000000 loops, best of 3: 0.0172 usec per loop
100000000 loops, best of 3: 0.022 usec per loop
100000000 loops, best of 3: 0.0191 usec per loop
10000000 loops, best of 3: 0.0208 usec per loop
10000000 loops, best of 3: 0.0144 usec per loop
10000000 loops, best of 3: 0.0201 usec per loop
100000000 loops, best of 3: 0.0195 usec per loop
100000000 loops, best of 3: 0.0231 usec per loop
10000000 loops, best of 3: 0.0149 usec per loop
100000000 loops, best of 3: 0.0211 usec per loop #数据有点不稳定，浮动较大，但是最要在0.016-0.019  机器负载曾经长时间在1.01,最高未超过1.1 （闲暇0.6左右）
</pre></figure></notextile></div></notextile></div>

<h4 id="section-6">一些我的看法</h4>

<p>虽然我的实验有不够严禁的地方，我的代码能力也有限（希望有朋友看见代码能提供修改意见或更NB的版本），但是效果还是比较明显的，我总结下</p>

<ol>
  <li>gevent确实性能很好，并且很稳定，占用io一般(据说长时间使用有内存泄露的问题？我不理解)</li>
  <li>scrapy这个框架把爬虫封装的很好，只需要最少的代码就能实现，性能也不差gevent</li>
  <li>多线程编程确实有瓶颈，并且不稳定</li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/djangohe-flaskfen-ye/">Django和flask分页</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-29T15:37:00+08:00" pubdate data-updated="true">Dec 29<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>最近搞了一些关于flask和django的东西，尤其是django的模板和admin功能以及这些框架使用bootstrap的东西，没时间更新博客，先说一下flask和django分页吧</p>

<h5 id="flaskbootstrapflask-paginatehttppackagespythonorgflask-paginate">flask的bootstrap分页插件<a href="http://packages.python.org/Flask-paginate">flask-paginate</a></h5>

<p>其实安装很常规，他的思路就是根据你的数据量给每个页面加一个li前缀到最后返回的div里面。因为官网提供的说明很简单，我在这里仔细说说：</p>

<ol>
  <li>官网说给你的网站页面添加css：</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
.pagination-page-info {
    padding: .6em;
    padding-left: 0;
    width: 40em;
    margin: .5em;
    margin-left: 0;
    font-size: 12px;
}
.pagination-page-info b {
    color: black;
    background: #6aa6ed;
    padding-left: 2px;
    padding: .1em .25em;
    font-size: 150%;
}
</pre></figure></notextile></div></notextile></div>
<p>其实这个是给你页面显示统计数据的方法pagination.info提供的样式，默认的class=’pagination’是bootstrap自带的，不需要你添加</p>

<ol>
  <li>官网的例子使用的是：Blueprint：</li>
</ol>

<p>我们一般都是： ‘from flask import Flask’，其实Blueprint就是一个可定制的容器，一个应用可以有多个容器，他们都继承于flask.helpers._PackageBoundObject
可以看我的一个例子：</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
@app.route('/')
def index():

	pagesize = 100 #设定每页显示条目数
	page = int(request.args.get('page',0)) #获取当前页面页数
	data = get_MongoData(page, pagesize) #get_MongoData是我自己的函数，根据页数过滤要显示的数据（因为实在太大了）
	pagination = Pagination(total=data[1], per_page=pagesize, page=page) #total的值是总数据条目，per_page表示每页显示数目，page就是当前页数。还可以设置向前/后页面标签（默认是&lt;&lt;/&gt;&gt;）等
	return render_template("index.html", pagination=pagination)
</pre></figure></notextile></div></notextile></div>

<ol>
  <li>我对他的一点修改：
    <ol>
      <li>我发现在我的程序里面，这个分页栏在后部会放不下而换行显示，我就直接把link_css制定的div改成了行内元素span</li>
      <li>当我默认使用link_size,代码是这样：</li>
    </ol>
    <xmp>
     link_css = &#8217;<span class="pagination{0} green"><ul>&#8217;
     其实最后页面出来的效果是&#8217;<span class="paginationNone green"><ul>&#8217; 
 
		
     这样就没有符合的bootstrap类，所以我修改了links方法:

     <div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
     @property
     def links(self):
         '''get all the pagination links'''
         if self.total_pages &lt;= 1:
             return ''
         if not self.link_size:
             self.link_size = ''
         s = [link_css.format(self.link_size)]
         s.append(self.prev_page)
         for page in self.pages:
             s.append(self.single_page(page) if page else gap_marker)

         s.append(self.next_page)
         s.append('')
         return ''.join(s)
     </pre></figure></notextile></div></notextile></div>

</ul></span></ul></span></xmp>
  </li>
</ol>
<p>#####flask的bootstrap分页插件<a href="http://tgdn.github.com/django-bootstrap-pagination/">django-bootstrap-pagination</a></p>

<p>django的插件比较复杂，它自己定义了中间件和标签，这样你需要在模板中load它提供的函数，并且很nb的使用了RequestContext去处理变量,可以看张沈鹏以前写的一个小文章：<a href="http://zsp.iteye.com/blog/115254">django 简化 view 函数的编写</a></p>

<ol>
  <li>先看我的后台方法：</li>
</ol>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">

def showlist(req):

	t = req.GET.get('type', None)
	l = req.GET.get('app', None)
	if t and l:
		db = getMongo('XXX.XXX.XXX.XXX:XX', 'dc2')
		if t == 'v':
			q = re.compile(r'.*%s$' % l)
			data = db.site.find({'modules.site.level':'v4', 'site':{ '$regex' : q }}, 
				{'site':1, '_id':0, 'modules.site.links':1, 'modules.site.keywords':1}).sort(
				'modules.site.site.check_time')

	return render_to_response("list.html", {'data':data}, context_instance=RequestContext(req))
</pre></figure></notextile></div></notextile></div>

<p>但是运行时候会报错：</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
Traceback:
File "/usr/local/lib/python2.6/dist-packages/django/core/handlers/base.py" in get_response
  111.                         response = callback(request, *callback_args, **callback_kwargs)
File "/home/dongwm/centerCon/views.py" in showlist
  68. 	return render_to_response("list.html", {'data':data}, context_instance=RequestContext(req))
File "/usr/local/lib/python2.6/dist-packages/django/shortcuts/__init__.py" in render_to_response
  20.     return HttpResponse(loader.render_to_string(*args, **kwargs), **httpresponse_kwargs)
File "/usr/local/lib/python2.6/dist-packages/django/template/loader.py" in render_to_string
  176.         return t.render(context_instance)
File "/usr/local/lib/python2.6/dist-packages/django/template/base.py" in render
  140.             return self._render(context)
File "/usr/local/lib/python2.6/dist-packages/django/template/base.py" in _render
  134.         return self.nodelist.render(context)
File "/usr/local/lib/python2.6/dist-packages/django/template/base.py" in render
  823.                 bit = self.render_node(node, context)
File "/usr/local/lib/python2.6/dist-packages/django/template/debug.py" in render_node
  74.             return node.render(context)
File "/home/dongwm/centerCon/templatetags/pagination_tags.py" in render 
  91.             page_obj = paginator.page(context['request'].page)
File "/usr/local/lib/python2.6/dist-packages/django/template/context.py" in __getitem__
  54.         raise KeyError(key)

Exception Type: KeyError at /showlist/
Exception Value: 'request'
</pre></figure></notextile></div></notextile></div>

<p>不管你用那个插件都会有这个报错。。。</p>

<p><em>后来发现原因是：</em></p>

<p><em>settings文件没有设置TEMPLATE_CONTEXT_PROCESSORS</em>
理由：模板上下文处理器会指定了哪些contextprocessors总是默认被使用。这样就省去了每次使用RequestContext都指定processors的麻烦
在settings加入：
TEMPLATE_CONTEXT_PROCESSORS = (
    “django.core.context_processors.media”,
    “django.core.context_processors.request”
    )</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/octopress-de-3D-biao-qian-yun/">octopress的3D标签云插件</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-11-17T14:22:00+08:00" pubdate data-updated="true">Nov 17<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>最近看了《社交网站的数据挖掘与分析》，了解到关于谷歌可视化工具，正好有2次都说到了标签的3D展现，以前用wordpress的时候有个插件叫做wp-cumulus，而octopress里面有一个相应的标签插件，但是却是静态展示，一直不爽，然后就萌发了改一个3D的octoress标签云插件的想法，其中word cumulus借用了<a href="https://www.google.com/jsapi">谷歌api</a>，ruby程序还是借以前的<a href="https://github.com/tokkonopapa/octopress-tagcloud">topress-tagcloud</a>，思路参考了<a href="http://code.google.com/p/word-cumulus-goog-vis">word-cumulus-goog-vis</a>,我的设计是这样的如下：</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">

1. jekyll生成静态页面的时候，根据相关插件计算标签的地址，数量和标签内容
2. 将这些数据用json.dump的方式写入到一个json文件（因为octopress是静态页面，不能从数据库抓取数据）
3. 新增一个javascript脚本（需要添加到source/_includes/head.html，具体看我的后面的例子程序中的注释），实现调取谷歌可视化工具的接口，把数据写到swf文件中
4. 当打开网页的时候会调用jquery的getjson（我用的是.ajax）读取数据，将数据格式化，通过js脚本写入div（div所在的html已经放在边栏）


</pre></figure></notextile></div></notextile></div>

<h5 id="section-1">插件代码如下</h5>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
# encoding: utf-8

require 'json' #导入json依赖
require 'pathname'
  

module Jekyll 

  class TagCloud &lt; Liquid::Tag  #标签云的类继承至Liquid::Tag，liquid是一个ruby的模版引擎，

    def initialize(tag_name, markup, tokens)
      @opts = {}
      if markup.strip =~ /\s*counter:(\w+)/i  #检查是否定义参数，没有的话不计算标签数量
        @opts['counter'] = ($1 == 'true') #哈希项的结果就是这个参数是否为true的布尔值
        markup = markup.strip.sub(/counter:\w+/i,'')
      end
      super
    end

    def render(context)
      lists = {}
      max, min = 1, 1
      config = context.registers[:site].config #内置检查站点配置
      category_dir = config['root'] + config['category_dir'] + '/' #标签基目录
      categories = context.registers[:site].categories
      categories.keys.sort_by{ |str| str.downcase }.each do |category| #标签根据小写字符串排序 挨个计算其存在数量
        count = categories[category].count
        lists[category] = count
        max = count if count &gt; max
      end

      li = []
      lists.each do | category, counter |
        nli = []
        url = category_dir + category.gsub(/_|\P{Word}/, '-').gsub(/-{2,}/, '-').downcase
        nli[0] = category + '[' + categories[category].count.to_s + ']' #第一个参数是标签和数量的字符串
        nli[1] = url #第二个标签是标签集合的地址
        if @config['category_counter']
          nli[2] = categories[category].count
        else nli[2] = 8
        end
        li.push(nli)
      end
      f = File.open('%s/../source/javascripts/tag.json' % \
          Pathname.new(File.dirname(__FILE__)).realpath,'w') #操作文件
      f.puts(JSON.dump(li)) #写入数据
      f.close()
    end
  end

end

Liquid::Template.register_tag('tag_cloud', Jekyll::TagCloud)  #让标签生效


</pre></figure></notextile></div></notextile></div>

<h5 id="javascript">javascript代码如下</h5>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">

var json_data = (function () {
    var json = null;
    $.ajax({   //通过jquery方法获取json文件的数据
        'async': false,
        'global': false,
        'url': '/javascript/tag.json',  //这里地址是错误的，因为静态页面会把握的标签当成真实环境，其实是javascripts，具体代码请看github项目
        'dataType': "json",
        'success': function (data) {
            json = data;
        }
    });
    return json;
})(); 
      google.setOnLoadCallback(drawVisualization);  //设定可视化load后调用函数

    function drawVisualization() {

        var data = new google.visualization.DataTable(); //创建数据表
        data.addColumn('string', 'Tag'); //加三个字段
        data.addColumn('string', 'URL'); 
        data.addColumn('number', 'Font size');

        data.addRows(json_data.length);  //确定标签的数量
        for (var i = 0; i &lt; json_data.length; i++) {
            var url = window.location.href + json_data[i][1];
            data.setCell(i, 0, json_data[i][0]); // 标签
            data.setCell(i, 1, url); // 标签的真实集合url
            data.setCell(i, 2, 2+1.5*json_data[i][2]); // 标签字体大小
        }

        var vis = new gviz_word_cumulus.WordCumulus(document.getElementById('tag-clouds'));  //找到id为‘tag-clou’的div，这里数据也有问题，原因如上

        vis.draw(data, {text_color: '#00ff00', speed: 50, width:220, height:220});  //画图  注意，修改效果云的大小在这里制定 我这里宽高都是220px
       }

</pre></figure></notextile></div></notextile></div>

<h5 id="htmlsourceincludescustomasidescategorycloudhtml">被包含的html代码如下(我放在了source/_includes/custom/asides/category_cloud.html)</h5>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
<xmp>
<section class="well">  //这个主要根据你的主题而定吧，我的主题右边栏的风格都是这样
   <ul id="gh_repos" class="nav">
    <li class="nav-header">标签Cloud</li>
  </ul>
  <div id="tag-clou"></div>   // 制定counter为true就会根据你的标签符合的文章数对画图效果显示的该标签字体大小比例而定，不指定或者制定其他值都按照一个字体大小显示所有标签
</section>
</xmp>        
        
并且将这个html放在默认的右边栏的配置中，修改_config.yaml，其中：

default_asides: [asides/recent_posts.html, custom/asides/links.html, asides/github.html, asides/delicious.html, asides/pinboard.html, asides/googleplus.html,custom/asides/category_cloud.html, custom/asides/douban.html, asides/article_total_sidebar.html]            


</pre></figure></notextile></div></notextile></div>

<h5 id="htmlheadhtmlsourceincludesheadhtmlfoothtmljquery">剩下就是修改你的主页的html（我直接修改了head.html[source/_includes/head.html],愿意的话你可以修改源码的foot.html甚至其它，只要保证它在jquery加载之后加载就好，我只粘贴新增的一部分和它附近的内容）</h5>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">

<xmp>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>  //原来就有
  <script type="text/javascript" src="/javascripts/api.js"></script>  //这里是谷歌的jsapi，我直接保存在我脚本里面，因为谷歌可能访问不稳定，你懂得
  <script type="text/javascript" src="/javascripts/wordcumulus.js"></script> //这是操作word-cumulus的
  <script type="text/javascript" src="/javascripts/swfobject.js"></script> //这里是操作wp-cumulus的flash文件的
  <script type="text/javascript" src="/javascripts/tagcumulus.js"></script> //这是我新建的上述js脚本地址
  <link href="/atom.xml" rel="alternate" title="小明明s à domicile" type="application/atom+xml" /> //原来就有
</xmp>

</pre></figure></notextile></div></notextile></div>

<p>打包相关文件以及详情请参看我的github项目：<a href="https://github.com/dongweiming/octopress-wp_cumulus_for_tagcloud">octopress-wp_cumulus_for_tagcloud</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/pa-chong-lian-xi/">爬虫练习</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-23T11:01:00+08:00" pubdate data-updated="true">Oct 23<span>rd</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>20号参加pycon,发现有个招聘公司<a href="http://blog.knownsec.com/2012/02/knownsec-recruitment/">知道创宇</a>, 正好换工作,就去公司网站转了下,发现挺有意思:投简历需要一个网站爬虫程序,基本要求如下(可以直接点开上面网页去看):</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_bash">
使用python编写一个网站爬虫程序，支持参数如下：

spider.py -u url -d deep -f logfile -l loglevel(1-5)  --testself -thread number --dbfile  filepath  --key=”HTML5”


参数说明：

-u 指定爬虫开始地址

-d 指定爬虫深度

--thread 指定线程池大小，多线程爬取页面，可选参数，默认10

--dbfile 存放结果数据到指定的数据库（sqlite）文件中

--key 页面内的关键词，获取满足该关键词的网页，可选参数，默认为所有页面

-l 日志记录文件记录详细程度，数字越大记录越详细，可选参数，默认spider.log

--testself 程序自测，可选参数
 
功能描述：

1、指定网站爬取指定深度的页面，将包含指定关键词的页面内容存放到sqlite3数据库文件中

2、程序每隔10秒在屏幕上打印进度信息

3、支持线程池机制，并发爬取网页

4、代码需要详尽的注释，自己需要深刻理解该程序所涉及到的各类知识点

5、需要自己实现线程池
</pre></figure></notextile></div></notextile></div>

<p>搞了2天,根据研究,弄了一个版本(友情提示,仅供学习参考,要是面试这个职位,建议大家用其它方法实现,因为我投递过了,不要拿来主义额^.^)
#####代码如下(隐藏了个人信息用’XXX’代替)</p>

<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
#!/usr/bin/env python
#coding=utf-8

import urllib2 
import Queue
import sys
import traceback
import threading
import re
import datetime
import lxml
import chardet
import logging
import logging.handlers
from time import sleep
from urlparse import urlparse
from lxml import etree
from optparse import OptionParser

try:
    from sqlite3 import dbapi2 as sqlite
except:
    from pysqlite2 import dbapi2 as sqlite 

#__doc__注释  执行脚本 -h 或者 --help  打印输出的内容
'''
This script is used to crawl analyzing web!

The Feature: 
1 可以指定抓取的深度
2 将抓取到的关键字数据存放在sqlite
3 使用logging记录日志
4 并发线程池

Required dependencies: 
1 chardet #分析抓取页面的字符集 
sudo easy_install chardet

Usage: 
spider.py -u url -d deep -f logfile -l loglevel(1-5)  --testself -thread number --dbfile  filepath  --key=”HTML5”

Writer: Dongweiming
Date: 2012.10.22
'''


lock = threading.Lock() #设置线程锁
LOGGER = logging.getLogger('Crawler') #设置logging模块的前缀
LEVELS={   #日志级别
        1:'CRITICAL',
        2:'ERROR',
        3:'WARNING',
        4:'INFO',
        5:'DEBUG',#数字越大记录越详细
        }
formatter = logging.Formatter('%(name)s %(asctime)s %(levelname)s %(message)s') #自定义日志格式

class mySqlite(object):
    
    def __init__(self, path, logger, level):
        '''初始化数据库连接.
 
           &gt;&gt;&gt; from sqlite3 import dbapi2 as sqlite
           &gt;&gt;&gt; conn = sqlite.connect('testdb')
        '''
        try:
            self.conn = sqlite.connect(path) #连接sqlite
            self.cur = self.conn.cursor()  #cursor是一个记录游标，用于一行一行迭代的访问查询返回的结果
        except Exception, e:
            myLogger(logger, self.loglevel, e, True)
            return -1
        
        self.logger = logger
        self.loglevel = level

    def create(self, table): 
        '''创建table，我这里创建包含2个段 ID（数字型，自增长），Data（char 128字符）'''
        try:
            self.cur.execute("CREATE TABLE IF NOT EXISTS %s(Id INTEGER PRIMARY KEY AUTOINCREMENT, Data VARCHAR(40))"% table)
            self.done()
        except sqlite.Error ,e: #异常记录日志并且做事务回滚,以下相同
            myLogger(self.logger, self.loglevel, e, True) 
            self.conn.rollback()
        if self.loglevel &gt;3: #设置在日志级别较高才记录,这样级别高的详细
                myLogger(self.logger, self.loglevel, '创建表%s' % table)

    def insert(self, table, data):
        '''插入数据，指定表名，设置data的数据'''
        try:
            self.cur.execute("INSERT INTO %s(Data) VALUES('%s')" % (table,data))
            self.done()
        except sqlite.Error ,e:
            myLogger(self.logger, self.loglevel, e, True)
            self.conn.rollback()
        else:
            if self.loglevel &gt;4:
                myLogger(self.logger, self.loglevel, '插入数据成功')

    def done(self):
        '''事务提交'''
        self.conn.commit()

    def close(self):
        '''关闭连接'''
        self.cur.close()
        self.conn.close()
        if self.loglevel &gt;3:
            myLogger(self.logger, self.loglevel, '关闭sqlite操作')


class Crawler(object):

    def __init__(self, args, app, table, logger):
        self.deep = args.depth  #指定网页的抓取深度        
        self.url = args.urlpth #指定网站地址
        self.key = args.key #要搜索的关键字
        self.logfile = args.logfile #日志文件路径和名字
        self.loglevel = args.loglevel #日志级别
        self.dbpth = args.dbpth #指定sqlite数据文件路径和名字
        self.tp = app #连接池回调实例
        self.table = table #每次请求的table不同 
        self.logger = logger #logging模块实例
        self.visitedUrl = [] #抓取的网页放入列表,防止重复抓取

    def _hasCrawler(self, url): 
        '''判断是否已经抓取过这个页面'''
        return (True if url in self.visitedUrl else False)
     
    def getPageSource(self, url, key, deep): 
        ''' 抓取页面,分析,入库.
        '''
        headers = {  #设计一个用户代理,更好防止被认为是爬虫
            'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; \
            rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6' }
        #if  urlparse(url).scheme == 'https':
           #pass
        if self._hasCrawler(url): #发现重复直接return
            return
        else:
            self.visitedUrl.append(url) #发现新地址假如到这个列表
        try:
            request = urllib2.Request(url = url, headers = headers) #创建一个访问请求,指定url,并且把访问请求保存在request 
            result = urllib2.urlopen(request).read() #打开这个请求,并保存读取数据
        except urllib2.HTTPError, e:  #触发http异常记录日志并返回
            myLogger(self.logger, self.loglevel, e, True)
            return -1
        try:
            encoding = chardet.detect(result)['encoding'] #判断页面的编码
            if encoding.lower() == 'gb2312':
                encoding = 'gbk'  #今天我抓取新浪是gb2312,但是其中有个'蔡旻佑'不能被识别,所以用gbk去解码gb2312的页面
            if encoding.lower() != 'utf-8': #发现不是默认编码就用应该的类型解码
                result = result.decode(encoding)
        except Exception, e:
            myLogger(self.logger, self.loglevel, e, True)
            return -1
        else:
            if self.loglevel &gt;3:
                myLogger(self.logger, self.loglevel, '抓取网页 %s 成功' % url)
        try:
            self._xpath(url, result, ['a'], unicode(key, 'utf8'), deep) #分析页面中<a>的连接地址,以及它的内容
            self._xpath(url, result, ['title', 'p', 'li', 'div'], unicode(key, "utf8"), deep) #分析这几个标签的内容
        except TypeError: #对编码类型异常处理,有些深度页面和主页的编码不同
            self._xpath(url, result, ['a'], key, deep)
            self._xpath(url, result, ['title', 'p', 'li', 'div'], key, deep)
        except Exception, e:
            myLogger(self.logger, self.loglevel, e, True)
            return -1
        else:
            if self.loglevel &gt;3:
                myLogger(self.logger, self.loglevel, '分析网页 %s 成功' % url)
        return True

    def _xpath(self, weburl, data, xpath, key, deep):

        sq = mySqlite(self.dbpth, self.logger, self.loglevel)
        page = etree.HTML(data)
        for i in xpath:
            hrefs = page.xpath(u"//%s" % i) #根据xpath标签
            if deep &gt;1:
                for href in hrefs:
                    url = href.attrib.get('href','')
                    if not url.startswith('java') and not  \
                        url.startswith('mailto'):  #过滤javascript和发送邮件的链接
                            self.tp.add_job(self.getPageSource,url, key, deep-1) #递归调用,直到符合的深度
            for href in hrefs:
                value = href.text  #抓取相应标签的内容
                if value:
                    m = re.compile(r'.*%s.*' % key).match(value) #根据key匹配相应内容
                    if m:
                        sq.insert(self.table, m.group().strip()) #将匹配的数据插入到sqlite
        sq.close()

    def work(self):
        '''主方法调用.
        
        &gt;&gt;&gt; import datetime
        &gt;&gt;&gt; logger = configLogger('test.log')
        &gt;&gt;&gt; time = datetime.datetime.now().strftime("%m%d%H%M%S")
        &gt;&gt;&gt; sq = mySqlite('test.db', logger, 1)
        &gt;&gt;&gt; table = 'd' + str(time)
        &gt;&gt;&gt; sq.create(table)
        &gt;&gt;&gt; tp = ThreadPool(5)
        &gt;&gt;&gt; def t():pass
        &gt;&gt;&gt; t.depth=1
        &gt;&gt;&gt; t.urlpth='http://www.baidu.com'
        &gt;&gt;&gt; t.logfile = 'test.log'
        &gt;&gt;&gt; t.loglevel = 1
        &gt;&gt;&gt; t.dbpth = 'test.db'
        &gt;&gt;&gt; t.key = 'test'
        &gt;&gt;&gt; d = Crawler(t, tp, table, logger)
        &gt;&gt;&gt; d.getPageSource(t.urlpth, t.key, t.depth)
        True
        '''
        if not self.url.startswith('http://'): #支持用户直接写域名,当然也支持带前缀
            self.url = 'http://' + self.url
        self.tp.add_job(self.getPageSource, self.url, self.key, self.deep)
        self.tp.wait_for_complete() #等待线程池完成


class MyThread(threading.Thread):

    def __init__(self, workQueue, timeout=30, **kwargs):
        threading.Thread.__init__(self, kwargs=kwargs)
        self.timeout = timeout #线程在结束前等待任务队列多长时间
        self.setDaemon(True)  #设置deamon,表示主线程死掉,子线程不跟随死掉
        self.workQueue = workQueue
        self.start() #初始化直接启动线程
 
    def run(self):
        '''重载run方法'''
        while True:
            try:
                lock.acquire()   #线程安全上锁
                callable, args = self.workQueue.get(timeout=self.timeout) #从工作队列中获取一个任务
                res = callable(*args)  #执行的任务
                lock.release()  #执行完,释放锁
            except Queue.Empty: #任务队列空的时候结束此线程
                break
            except Exception, e:
                myLogger(self.logger, self.loglevel, e, True)
                return -1


class ThreadPool(object):

    def __init__(self, num_of_threads):
         self.workQueue = Queue.Queue()
         self.threads = []
         self.__createThreadPool(num_of_threads)
 
    def __createThreadPool(self, num_of_threads):
         for i in range(num_of_threads):
             thread = MyThread(self.workQueue)
             self.threads.append(thread)
 
    def wait_for_complete(self):
         '''等待所有线程完成'''
         while len(self.threads):
             thread = self.threads.pop()
         if thread.isAlive():  #判断线程是否还存活来决定是否调用join
             thread.join()
     
    def add_job( self, callable, *args):
        '''增加任务,放到队列里面'''
        self.workQueue.put((callable, args))
   

def configLogger(logfile):
    '''配置日志文件和记录等级'''
    try:
        handler = logging.handlers.RotatingFileHandler(logfile, 
                                                       maxBytes=10240000, #文件最大字节数
                                                       backupCount=5, #会轮转5个文件，共6个
                                                        )
    except IOError, e:
        print e
        return -1
    else:
        handler.setFormatter(formatter)  #设置日志格式
        LOGGER.addHandler(handler) #增加处理器
        logging.basicConfig(level=logging.NOTSET) #设置,不打印小于4级别的日志
    return LOGGER #返回logging实例

def myLogger(logger, lv, mes, err=False):
    '''记录日志函数'''
    getattr(logger, LEVELS.get(lv, 'WARNING').lower())(mes)
    if err: #当发现是错误日志,还会记录错误的堆栈信息
        getattr(logger, LEVELS.get(lv, 'WARNING').lower())(traceback.format_exc())

def parse():
    parser = OptionParser(
                  description="This script is used to crawl analyzing web!")
    parser.add_option("-u", "--url", dest="urlpth", action="store",
                  help="Path you want to fetch", metavar="www.sina.com.cn")
    parser.add_option("-d", "--deep", dest="depth", action="store",type="int",
                  help="Url path's deep, default 1", default=1)
    parser.add_option("-k", "--key", dest="key", action="store",
                  help="You want to query keywords, For example 'test'")
    parser.add_option("-f", "--file", dest="logfile", action="store",
                  help="Record log file path and name, default spider.log", 
                  default='spider.log')
    parser.add_option("-l", "--level", dest="loglevel", action = "store",
                  type="int",help="Log file level, default 1(CRITICAL)", 
                  default=1)
    parser.add_option("-t", "--thread", dest="thread", action="store",
                  type="int",help="Specify the thread pool, default 10", 
                  default=10)
    parser.add_option("-q", "--dbfile", dest="dbpth", action="store",
                  help="Specify the the sqlite file directory and name, \
                  default  test.db", metavar='test.db')
    parser.add_option("-s", "--testself", dest="testself", action="store_true",
                  help="Test myself", default=False)
    (options, args) = parser.parse_args()  
    return options

def main():
    '''主函数'''
        
    options = parse()
    if options.testself: #如果testself,执行doctest
        import doctest
        print doctest.testmod()
        return
    if not options.urlpth or not options.key or not options.dbpth: #判断必选项是否存在
        print 'Need to specify the parameters option "-u " or "-k" or "-q"!'
        return
    if '-h' in sys.argv  or '--help' in sys.argv:  #选择帮助信息,打印__doc__
        print __doc__

    logger = configLogger(options.logfile) #实例化日志调用
    time = datetime.datetime.now().strftime("%m%d%H%M%S") #每次请求都会根据时间创建table
    tp = ThreadPool(options.thread) 
    sq = mySqlite(options.dbpth, logger, options.loglevel)
    table = 'd' + str(time)
    sq.create(table) #创建table
    sq.close()
    crawler = Crawler(options, tp, table, logger)
    crawler.work()  #主方法
 
if __name__ == '__main__':
    main()

</a></pre></figure></notextile></div></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/pythonban-ge-ren-jian-li/">Python版个人简历</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-21T14:24:00+08:00" pubdate data-updated="true">Oct 21<span>st</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>最近换工作,无聊之下搞了个python版本的简历,包含我的相应信息,以及使用了一个装饰器用来显示颜色和相应块信息.并上传到gist.github.com</p>

<h5 id="xxx">代码如下(隐藏了个人信息用’XXX’代替)</h5>
<div class="bogus-wrapper"><notextile><div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_python">
#/usr/bin/env python
# -*- coding: utf-8 -*-

import random
import re


def color(messages):
    color = "\x1B[%d;%dm" % (1, random.randint(30,37))
    return "%s %s\x1B[0m" % (color, messages)

def colorprint(mes, flag=True):
    def _mydecorator(func):
        def _mydecorator(*args):
            res = func(*args)
            print color(mes+":"),"\n"
            if flag:
                for k1, v1 in res.items():
                    if not isinstance(v1, dict):
                        print "{0}: {1}".format(k1.rjust(16), v1)
                    else:
                        print "{0}:".format(k1.rjust(16))
                        for k2, v2 in v1.items():
                            print "{0}: {1}".format(k2.rjust(20), v2)
            else:
                for i in res:
                    if not isinstance(i[1], dict):
                        print i
                    else:
                        for k, v in i[1].items():
                            print "{0}[{1}]: {2}".format(k.rjust(16), \
                    i[0], v)
            return res
        return _mydecorator
    return _mydecorator

class ForJob(object):

    def __str__(self):
        return color("XXX的python简历".center(400))

    @property
    @colorprint("个人信息")
    def personal_information(self):
        return {
          "Name" : "XXX",
                  "Gender" : "Male",
                  "Born" : ['19XX', 'X', 'X'],
          "Education" : {
              "School Name" : "保定科技职业学院",
              "Major" : "烹饪工艺与营养",
              "Degree" : "Three-year college",
              "Graduation" : 2009},
                  "QQ" : 61966225,
          "Tel" : 13552651322,
          "Email" : "ciici1234@hotmail.com",
          "Target Positions" : re.compile("'Python Developer'|Architect|DevOps",re.I|re.M).pattern}
        
    @property
    @colorprint("个人特点")
    def characteristics(self):
        return {
            "心里承受能力强" : "从非计算机专业―思科方向\
―linux运维―C/python开发",
            "对计算机技术的热衷和喜爱" : "正是因为喜欢IT,\
我才会放弃大学专业",
            "自学能力强" : "没有大学的计算机基础除思科参加培训，\
其它都为都是自学",
            "毅力和耐性" : "从不放弃一个解决不了的难题，\
看过的计算机专业技术多于700页的书籍&gt;30本",
            "is_geek" : True}

    @property
    @colorprint("个人能力")
    def skills(self):
        return {
            "Language" : {
                "熟悉" : ["Python", "Ruby", "Bash", "c"],
                "了解" : ["Haskell", "Lisp", "Erlang"]},
            "OS" : ["Gentoo", "Debian", "Centos/Rhel", "Opensuse"],
            "Tool" : ["Vim", "Mercurial", "Git"],
            "Databaseandtools" : ["Mysql",
                "Postgresql", "Mongodb", "Redis", "Memcached", "Sqlalchemy"],
            "WebFramework" : {
                "熟悉" : ["Tornado", "Django", "Gae"],
                "了解" : ["Flask"]},
            "OtherFramework" : ["Twisted", "gevent",
               "stackless", "scrapy", "mechanize"],
            "GUI" : "pyqt",
            "Network" : "Cisco Certified Security Professional",
"Other" : "给gentoo和centos提交过bug"}

    @property
    @colorprint("工作经验",False)
    def work_experience(self):
        return enumerate([
            {
                "Time period" : "2009.09-2011.08",
                "Company Name" : "XXX（北京）科技股份有限公司",
                "Position" : "高级运维工程师"},
            {
                "Time period" : "2011.09-2012.08",
                "Company Name" : "北京XXX有限责任公司",
                "Position" : "linux python研发工程师"}])

    @property
    @colorprint("项目经验",False)
    def project_experience(self):
        return enumerate([
            {
                "Project" : "kvm远程管理系统",
"Description" : "前台(django)接手至其它同事并完成维护，\
后台独立完成，用来创建，修改，删除kvm，查看状态信息等"},
            {
                "Project" : "postfix群发邮件系统",
"Description" : "前台(tornado),为其它部门提供发送邮件的web端, \
并作为数据收集服务端,前后台独立完成"},
            {
                "Project" : "windows个人安全终端系统",
"Description" : "前后台和接收数据的socket服务器独立完成，\
客户端图形编程使用qt"},
            {
                "Project" : "地推(一个分布在全国的各个办事处的称呼:地面推广) \
IDC质量测试系统",
                "Description": "还在代码实现中,前台flask, \
数据接收服务准备使用twisted,客户端为windows进程"}])
 
    @property
    @colorprint("@Where",False)
    def findme(self):
        return enumerate([
            {
                "Link" : "http://www.dongwm.com",
"Description" : "个人技术博客"},
            {
                "Link" : "http://www.zhihu.com/people/dongweiming",
"Description" : "知乎"},
            {
                "Link" : "http://www.quora.com/Weiming-Dong",
"Description" : "Quora"},
            {
                "Link" : r"https://twitter.com/#!/dongweiming",
"Description" : "Twitter"},
            {
                "Link" : "http://www.ailll.com",
"Description" : "音乐分享网站"},
            {
                "Link" : "http://dongwm.blog.51cto.com",
"Description" : "51cto的推荐博客"},
            {
                "Link" : "http://youhouer.appspot.com",
"Description" : "基于Google App Engine的前端网站"}])
                
    def show(self):
        prolist = [i for i in dir(self) if not i.startswith("_") \
            and not i.startswith("personal")]
        self.personal_information
        for pro in prolist:
            getattr(self, pro)

if __name__ == "__main__":
    dongweiming = ForJob()
    print dongweiming
    dongweiming.show()
</pre></figure></notextile></div></notextile></div>

<h5 id="section-1">效果:</h5>
<ol>
  <li>直接保存文件,linux下终端执行,但是windows执行会有乱码</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>直接linux下终端执行:curl https://raw.github.com/gist/3496061/b63efdf0c1c8d0e12df4f5b905903128c951b282/My_Resume.py</td>
          <td>python</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/qian-yi-octpressyi-ji-zi-ding-yi/">迁移octpress,Rakefile修改以及豆瓣推荐,豆瓣收藏秀,新浪微博分享按钮等实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-28T12:11:00+08:00" pubdate data-updated="true">Sep 28<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h4 id="section"><em>前言</em></h4>

<p>首先感谢吴钊的<a href="http://www.neoease.com/inove">inove</a>,从博客建立就一直用它,昨天就开始研究迁移成octopress,以下是一些经验和总结</p>

<h4 id="octopress">1. octopress目录结构说明</h4>

<h5 id="gitoctopress">首先看git出来的octopress目录结构:</h5>

<div class="bogus-wrapper"><notextile><figure class="code"><pre class="sh_"><span class="line">├─ config.rb  #指定额外的compass插件
</span><span class="line">├─ config.ru  
</span><span class="line">├─ Rakefile   #rake的配置文件,类似于makefile,这个我修改了一些内容
</span><span class="line">├─ Gemfile    #bundle要下载需要的gem依赖关系的指定文件
</span><span class="line">├─ Gemfile.lock  #这些gem依赖的对应关系,比如A的x本依赖于B的y版本,我也修改了
</span><span class="line">├─ _config.yml  #站点的配置文件
</span><span class="line">├─ public/  #在静态编译完成后的目录,网站只需要这个目录下的文件树
</span><span class="line">├─ _deploy/  #deploy时候生成的缓存文件夹,和public目录一样
</span><span class="line">├─ sass/  #css文件的源文件,过程中会compass成css
</span><span class="line">├─ plugins/  #放置自带以及第三方插件的目录,ruby程序
</span><span class="line">│  └── xxx.rb
</span><span class="line">└─ source/  #这个是站点的源文件目录,public目录就是根据这个目录下数据生成的
</span><span class="line">   └─ _includes/
</span><span class="line">      └─ custom/  #自定义的模板目录,被相应上级html include
</span><span class="line">         └─ asides/  #边栏模板自定义模板目录
</span><span class="line">      └─ asides/  #边栏模板目录
</span><span class="line">      └─ post/  #文章页面相应模板目录
</span><span class="line">   └─ _layouts/  #默认网站html相关文件,最底层
</span><span class="line">   └─ _posts/  #新增以及从其它程序迁移过来的数据都存在这里
</span><span class="line">   └─ stylesheets/ #css文件目录
</span><span class="line">   └─ javascripts/  #js文件目录</span></pre></figure></notextile></div>
</div>
  
  
    <footer>
      <a rel="full-article" href="/archives/qian-yi-octpressyi-ji-zi-ding-yi/">继续阅读 &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/shiyongoctopresszuoweigithub-comwangzhandebokekuangjia/">使用octopress作为github.com网站的博客框架</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-26T00:00:00+08:00" pubdate data-updated="true">Sep 26<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前言:使用ruby写的开源octopress越来越受到欢迎,并且因为github.com,很多作者把blog放在其二级域名上面,使用了它.国内也有很多人使用,比如mrzhang.me,其作者的主题还是不错滴,最近在华蟒python邮件组一封招聘邮件,里面竟然其中有一条:* 有github账号，开源项目和octpress的blog(但是职位是python的web开发,我很囧)</p>

<p><strong>1 安装前准备:</strong></p>

<p>假设你有github帐号,利用github提供的一个特性,使用key实现ssh信任链接</p>

<p>ssh-keygen -t dsa
cat ~/.ssh/id_dsa.pub#把其中的数据粘贴到github上面,这个就不说了</p>

<p><strong>2 使用rvm:</strong>
echo insecure &gt;&gt; ~/.curlrc
curl -k https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer | bash -s stable #ruby版本管理,类似于python的pythonbrew
export PATH=$PATH:/home/dongwm/.rvm/bin #或者直接写到profile
rvm list known #列出已知的ruby #本来我使用的ruby是1.8的 但是这个框架需要1.9.2以上?
ruby 1.8.7 (2011-06-30 patchlevel 352) [i686-linux]
dongwm@dongwm ~ $ rvm install 1.9.3 #安装ruby1.9.3</p>

<p>git clone git://github.com/imathis/octopress.git dongwm.github.com  #下载octopress源码
dongwm@dongwm ~ $ rvm  &#8211;create use 1.9.3@dongwm.github.com #使用ruby1.9.3
Using /home/dongwm/.rvm/gems/ruby-1.9.3-p194 with gemset dongwm.github.com
Running /home/dongwm/.rvm/hooks/after_use</p>

<p>dongwm@dongwm ~ $ cd dongweiming.github.com/  #第一次切换到这个放置octopress目录时会提示你以下信息
Do you wish to trust this .rvmrc file? (/home/dongwm/dongwm.github.com/.rvmrc)
y[es], n[o], v[iew], c[ancel]&gt; y  #选择 y  以后就不在问你了</p>

<p><strong>3 使用bundle管理项目中所有gem依赖</strong></p>

<p>dongwm@dongwm ~/dongwm.github.com $ gem install bundler</p>

<p>dongwm@dongwm ~/dongwm.github.com $ bundle install #安装需要的gem依赖</p>

<p><strong>4 安装主题,假如我们不想用默认的主题</strong></p>

<p>dongwm@dongwm ~/dongwm.github.com $ git clone git://github.com/bkutil/bootstrap-theme.git .themes/bootstrap-theme
#git clone git://github.com/sevenadrian/foxslide .themes/foxslide
#git clone git://github.com/barmstrong/octopress-bootstrap.git .themes/octopress-bootstrap
dongwm@dongwm ~/dongwm.github.com $ rake install[&#8216;foxslide&#8217;] #安装主题,默认主题rake install
dongwm@dongwm ~/dongwm.github.com $ rake generate #生成模板文件</p>

<p>注:每次换主题其实就是下载git源码+rake install + rake generate</p>

<p><strong>5 可选 代码预览</strong></p>

<p>假如测试环境想预览效果可以使用rake preview</p>

<p><strong>6 部署代码到github</strong></p>

<p>dongwm@dongwm ~/dongwm.github.com $ rake setup_github_pages  #设置链接
Enter the read/write url for your repository
(For example, &#8216;git@github.com:your_username/your_username.github.com)
Repository url: git@github.com:dongweiming/dongweiming.github.com  这里的dongweiming是我的帐号名字,后面的dongweiming.github.com是我的源,也就是github创建的源的名字,需要你手动在github网站增加,其实也是最后项直接访问的网站名字 以后访问 http://dongweiming.github.com
dongwm@dongwm ~/dongwm.github.com $ rake deploy #部署到github</p>

<p>当你看到“Github Pages deploy complete”后，就表示done,可以访问了</p>

<p><strong>7 版本控制</strong></p>

<p>既然是github,不用git就搞笑了</p>

<p>dongwm@dongwm ~/dongwm.github.com $ 832  cd source/_posts/ #因为在添加文章之类都会在source/_posts目录下面增加相应的文件,那么我要备份这个目录,也就是使用版本控制
dongwm@dongwm ~/octopress/source/_posts $ git init #初始化
dongwm@dongwm ~/octopress/source/_posts $ touch README.md
dongwm@dongwm ~/octopress/source/_posts $  git add *
dongwm@dongwm ~/octopress/source/_posts $ git commit -m &#8216;First version&#8217;</p>

<p>dongwm@dongwm ~/octopress/source/_posts $ git remote add dongwm git@github.com:dongweiming/dongweiming.github.com.git</p>

<p>这个意思就是 我添加了一个叫做&#8217;dongwm&#8217;的远程快捷方式,他链接到dongweiming帐号的dongweiming.github.com项目</p>

<p>dongwm@dongwm ~/octopress/source/_posts $ git checkout -b backup #创建一个分支叫做backup
Switched to a new branch &#8216;backup&#8217;
dongwm@dongwm ~/octopress/source/_posts $ git push dongwm backup #将修改push到backup分支,这样就实现了对这个目录的控制
Counting objects: 3, done.
Writing objects: 100% (3/3), 213 bytes, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:dongweiming/dongweiming.github.com.git
* [new branch]      backup -&gt; backup</p>

<p>&nbsp;</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/shiyongsphinxheapachedajianwikiwendangwangzhan/">使用sphinx和apache搭建wiki文档网站</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-13T00:00:00+08:00" pubdate data-updated="true">Sep 13<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前言：sphix是一个允许开发人员以纯文本格式使用reStructuredText 标记语法编写文档，自定义显示效果的文档工具。比较有代表性的网站有<a href="http://docs.python.org" target="_blank">python的docs官网</a>，<a href="http://book.42qu.com" target="_blank">张沈鹏的42区</a>等等，没事我在内网弄个记录一些工作文档。</p>

<p>注：我这里使用了gentoo系统</p>

<p><strong>1  安装sphinx和apache2</strong></p>

<p>因为还有个同名的大名鼎鼎的sphinx搜索引擎，需要指定到类型：</p>

<p>sudo emerge dev-python/sphinx apache2
<strong>2 配置一个wiki站点</strong></p>

<p>dongwm@localhost ~ $sphinx-quickstart  # 使用这个命令快速创建</p>

<p>&gt; Root path for the documentation [.]:    test #过程中会有一些提示选项，根据你的需要定制，我对所有的疑问都是yes，注意这步，是问你这个配置的站点的root目录，我这里是一个新的test子目录目录，在家目录下
完成后会出现test目录，包含这样的内容：</p>

<p>dongwm@localhost ~/test $ ls -l
total 24
drwxr-xr-x 4 dongwm dongwm 4096 Sep 14 13:38 build  #最后生成的html文件目录
-rw-r&#8211;r&#8211; 1 dongwm dongwm 5113 Sep 14 13:37 make.bat #我想是windows下的make
-rw-r&#8211;r&#8211; 1 dongwm dongwm 5589 Sep 14 13:37 Makefile #这个大家都熟悉
drwxr-xr-x 4 dongwm dongwm 4096 Sep 14 13:38 source  #源文件就是我们要编辑的wiki显示的代码的源文件目录</p>

<p><strong>3 修改要显示的源文件</strong>
原理：修改source下面的XXX.rst(或者你定义的其他后缀文件)，最后会生成同名的html文件，比如编辑index.rst会生成index.html，这个也就是默认的主显文件，通过他链接到其他文档处</p>

<p><strong>4 生成html文件目录</strong>
dongwm@localhost ~/test $  make html</p>

<p>这样就会在build/html目录下生成html文件，当然你也自定义</p>

<p><strong>5 设置apache以及htaccess</strong></p>

<p>修改/etc/apache2/vhosts.d/default_vhost.include</p>

<p>DocumentRoot &#8220;/home/dongwm/test/build/html&#8221;  12行 其中/home/dongwm/test/build/html就是生成html的目录，我就使用了默认的</p>

<p>&lt;Directory &#8220;/home/dongwm/test/build/html&#8221;&gt;  15行</p>

<p>在/home/dongwm/test/build/html目录，也就是网站根目录添加新文件.htaccess</p>

<p>dongwm@localhost ~/test/build/html $ cat !$
cat .htaccess
&lt;Files ~ &#8220;^.(htaccess|htpasswd)$&#8221;&gt;
deny from all
&lt;/Files&gt;
Options -Indexes
&lt;FilesMatch &#8220;.(gif|jpg|jpeg|png|ico)$&#8221;&gt;
Header set Cache-Control &#8220;max-age=86400&#8221;  #静态图片缓存24小时
&lt;/FilesMatch&gt;
AuthUserFile /home/dongwm/.htpasswd  #密码验证文件使用htpasswd生成
AuthGroupFile /dev/null
AuthName &#8220;Please enter your ID and password&#8221;
AuthType Basic
require valid-user
order deny,allow
allow from 10.28.101.1/24  #只容许这个段的人访问
deny from all</p>

<p>dongwm@localhost ~/test $ sudo /etc/init.d/apache2 restart  #重启apache2</p>

<p><strong>6 显示中文</strong></p>

<p>修改source/conf.py其中的language=&#8221;zh_CN&#8221; 重新make html即可</p>

<p><strong>7 技巧</strong></p>

<p>1 快速开发</p>

<p>每次我们修改了rst的源文件还需要make html去生成html文件，操作很麻烦，我在使用sublime_text2编辑器，写一个自定义编译程序的build，然后在Tools-&gt;Build System里面选择这个程序，</p>

<p>比如：dongwm@localhost ~ $ cat ~/.config/sublime-text-2/Packages/User/makehtml.sublime-build  #一定要保存在这个目录，这个命令就是makehtml（去掉文件的.sublime-build后缀）
{
&#8220;cmd&#8221;: [&#8220;make&#8221;, &#8220;html&#8221;], #我要执行make html  每个参数都要用引号隔开
&#8220;working_dir&#8221;: &#8220;${project_path:${folder}}&#8221;  #他的语法大家可以去看官方文档，这里表示执行这个命令实在我当前的工作目录
}</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/guanyugeventdeyixielijieyi-2/">关于gevent的一些理解(一)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-26T00:00:00+08:00" pubdate data-updated="true">Jul 26<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前言:gevent是python的一个并发框架,以微线程greenlet为核心，使用了epoll事件监听机制以及诸多其他优化而变得高效.而且其中有个monkey类,
将现有基于Python线程直接转化为greenlet(类似于打patch).他和线程框架性能比高大概4倍(看下图,是gevent和paste的对比):</p>

<p><img class="alignnone" title="性能" src="http://code.mixpanel.com/wp-content/uploads/2010/10/performance2.png" alt="" width="664" height="389" /></p>

<p>工作暂时没有用gevent的地方,这里就简单的对http://sdiehl.github.com/gevent-tutorial的一些例子和内容翻译:</p>

<p><strong>1 同步和异步</strong></p>

<p><pre class="sh_python">
import gevent</p>

<p>def foo():
    print('Running in foo')
    gevent.sleep(0) #让当前的greenlet睡眠N秒,这0标识控制其它协程而不会让其它进程睡眠
    print('Explicit context switch to foo again')</p>

<p>def bar():
    print('Explicit context to bar')
    gevent.sleep(0)
    print('Implicit context switch back to bar')</p>

<p>gevent.joinall([  #<tt>gevent.Greenlet</tt>实例,直到这个greenlet完成或者超时
    gevent.spawn(foo),  #spawn可以实现一个grennlet实例并且加到队列并且启动,效果类似于gevent.Greenlet(foo).start()
    gevent.spawn(bar),
])
</pre></p>

<p>执行结果的效果图:</p>

<p><img class="alignnone" title="效果" src="http://sdiehl.github.com/gevent-tutorial/flow.gif" alt="" width="284" height="277" /></p>

<p>dongwm@localhost ~ $ python test.py
Explicit context to bar
Running in foo
Explicit context switch to foo again
Implicit context switch back to bar</p>

<p><pre class="sh_python">
import time
import gevent
from gevent import select #类似于内置的<tt>select.select()</tt>实现(请关注http://www.dongwm.com/archives/guanyuselectyanjiu/),只是将线程操作改成了greenlet</p>

<p>start = time.time()
tic = lambda: 'at %1.1f seconds' % (time.time() - start)</p>

<p>def gr1():
    print('Started Polling: ', tic())
    select.select([], [], [], 2)  #参数分别是,等待的可读列表,等待的可写列表,等待的可执行列表,超时时间(这里是2秒)
    print('Ended Polling: ', tic())</p>

<p>def gr2():
    print('Started Polling: ', tic())
    select.select([], [], [], 2)
    print('Ended Polling: ', tic())</p>

<p>def gr3():
    print("Hey lets do some stuff while the greenlets poll, at", tic())
    gevent.sleep(1)</p>

<p>gevent.joinall([
    gevent.spawn(gr1),
    gevent.spawn(gr2),
    gevent.spawn(gr3),
])
</pre></p>

<p>执行结果:</p>

<p>dongwm@localhost ~ $ python test.py
(&#8216;Hey lets do some stuff while the greenlets poll, at&#8217;, &#8216;at 0.0 seconds&#8217;)  #因为gr1和gr2开始是阻塞的,gr3直接打印
(&#8216;Started Polling: &#8216;, &#8216;at 0.0 seconds&#8217;)
(&#8216;Started Polling: &#8216;, &#8216;at 0.0 seconds&#8217;)
(&#8216;Ended Polling: &#8216;, &#8216;at 2.0 seconds&#8217;)
(&#8216;Ended Polling: &#8216;, &#8216;at 2.0 seconds&#8217;)
<pre class="sh_python">
import gevent
import random</p>

<p>def task(pid):
    gevent.sleep(random.randint(0,2)*0.001)
    print('Task', pid, 'done')</p>

<p>def synchronous():  #同步
    for i in range(1,10):
        task(i)</p>

<p>def asynchronous(): #异步
    threads = [gevent.spawn(task, i) for i in xrange(10)]
    gevent.joinall(threads)</p>

<p>print('Synchronous:')
synchronous()</p>

<p>print('Asynchronous:')
asynchronous()
</pre></p>

<p>执行结果:</p>

<p>dongwm@localhost ~ $ python test.py
Synchronous:  #协程不会控制其它进程睡眠,所以挨个执行
(&#8216;Task&#8217;, 1, &#8216;done&#8217;)
(&#8216;Task&#8217;, 2, &#8216;done&#8217;)
(&#8216;Task&#8217;, 3, &#8216;done&#8217;)
(&#8216;Task&#8217;, 4, &#8216;done&#8217;)
(&#8216;Task&#8217;, 5, &#8216;done&#8217;)
(&#8216;Task&#8217;, 6, &#8216;done&#8217;)
(&#8216;Task&#8217;, 7, &#8216;done&#8217;)
(&#8216;Task&#8217;, 8, &#8216;done&#8217;)
(&#8216;Task&#8217;, 9, &#8216;done&#8217;)
Asynchronous:  #他们放在grennlet里面,sleep的时间是随机的,完成顺序也就不同了
(&#8216;Task&#8217;, 2, &#8216;done&#8217;)
(&#8216;Task&#8217;, 3, &#8216;done&#8217;)
(&#8216;Task&#8217;, 5, &#8216;done&#8217;)
(&#8216;Task&#8217;, 7, &#8216;done&#8217;)
(&#8216;Task&#8217;, 9, &#8216;done&#8217;)
(&#8216;Task&#8217;, 6, &#8216;done&#8217;)
(&#8216;Task&#8217;, 1, &#8216;done&#8217;)
(&#8216;Task&#8217;, 0, &#8216;done&#8217;)
(&#8216;Task&#8217;, 8, &#8216;done&#8217;)
(&#8216;Task&#8217;, 4, &#8216;done&#8217;)
<pre class="sh_python">
import gevent
from gevent import Greenlet</p>

<p>def foo(message, n):
    gevent.sleep(n)
    print(message)</p>

<p>thread1 = Greenlet.spawn(foo, "Hello", 1)  #实例化Greenlet
thread2 = gevent.spawn(foo, "I live!", 2) #实例化gevent,其实也是创建Greenlet实例,只是包装了一下
thread3 = gevent.spawn(lambda x: (x+1), 2)  #一个lambda表达式</p>

<p>threads = [thread1, thread2, thread3]
gevent.joinall(threads) #等待所有greenlet完成
</pre></p>

<p>执行结果:</p>

<p>dongwm@localhost ~ $ python test.py
Hello
I live!  #打印出来效果不明显,事实上等待一秒打印第一行,再等待一秒打印第二行,然后马上完成(lambda没有显示)</p>

<p><pre class="sh_python">
import gevent
from gevent import Greenlet</p>

<p>class MyGreenlet(Greenlet):  #重载Greenlet类</p>

<p>    def __init__(self, message, n):
        Greenlet.__init__(self)
        self.message = message
        self.n = n</p>

<p>    def _run(self): #重写_run方法
        print(self.message)
        gevent.sleep(self.n)</p>

<p>g = MyGreenlet("Hi there!", 3)
g.start()
g.join()
</pre></p>

<p><pre class="sh_python">
import gevent</p>

<p>def win():
    return 'You win!'</p>

<p>def fail():
    raise Exception('You fail at failing.')</p>

<p>winner = gevent.spawn(win)
loser = gevent.spawn(fail)</p>

<p>print(winner.started) # started表示的Greenlet是否已经开始,返回布尔值
print(loser.started)  # True</p>

<p>try:
    gevent.joinall([winner, loser])
except Exception as e:
    print('This will never be reached')</p>

<p>print(winner.value) # value表示greenlet实例返回值:'You win!'
print(loser.value)  # None</p>

<p>print(winner.ready()) # 是否已停止Greenlet的布尔值,True
print(loser.ready())  # True</p>

<p>print(winner.successful()) # 表示的Greenlet是否已成功停止，而不是抛出异常,True
print(loser.successful())  # False
print(loser.exception) #打印异常的报错信息
</pre></p>

<p>执行结果:</p>

<p>dongwm@localhost ~ $ python test.py
True
True
Traceback (most recent call last):
File &#8220;/usr/lib/python2.7/site-packages/gevent-1.0dev-py2.7-linux-i686.egg/gevent/greenlet.py&#8221;, line 328, in run
result = self._run(*self.args, **self.kwargs)
File &#8220;test.py&#8221;, line 7, in fail
raise Exception(&#8216;You fail at failing.&#8217;)
Exception: You fail at failing.
&lt;Greenlet at 0xb73cd39cL: fail&gt; failed with Exception</p>

<p>You win!
None
True
True
True
False
You fail at failing.
<pre class="sh_python">
import gevent
from gevent import Timeout</p>

<p>seconds = 10</p>

<p>timeout = Timeout(seconds)
timeout.start()</p>

<p>def wait():
    gevent.sleep(10)</p>

<p>try:
    gevent.spawn(wait).join()
except Timeout:
    print 'Could not complete'
</pre></p>

<p>上面的例子是可以执行完成的,但是假如修改seconds = 5,让数值少入sleep,那么就会有超时被捕捉到</p>

<p>还可以使用with关键字处理上下文:</p>

<p><pre class="sh_python">
import gevent
from gevent import Timeout</p>

<p>time_to_wait = 5 # seconds</p>

<p>class TooLong(Exception):
    pass</p>

<p>with Timeout(time_to_wait, TooLong):
    gevent.sleep(10)
</pre></p>

<p>以及其他的方式的:</p>

<p><pre class="sh_python">
import gevent
from gevent import Timeout</p>

<p>def wait():
    gevent.sleep(2)</p>

<p>timer = Timeout(1).start()
thread1 = gevent.spawn(wait)  #这种超时类型前面讲过</p>

<p>try:
    thread1.join(timeout=timer)
except Timeout:
    print('Thread 1 timed out')</p>

<p>timer = Timeout.start_new(1) #start_new是一个快捷方式
thread2 = gevent.spawn(wait)</p>

<p>try:
    thread2.get(timeout=timer) #get返回greenlet的结果,包含异常
except Timeout:
    print('Thread 2 timed out')</p>

<p>try:
    gevent.with_timeout(1, wait) #如果超时前返回异常,取消这个方法
except Timeout:
    print('Thread 3 timed out')
</pre></p>

<p><strong>2 数据结构</strong></p>

<p><pre class="sh_python">
import gevent
from gevent.event import AsyncResult</p>

<p>a = AsyncResult() #保存一个值或者一个异常的事件实例</p>

<p>def setter():
    gevent.sleep(3)  #3秒后唤起所有线程的a的值
    a.set() #保存值,唤起等待线程</p>

<p>def waiter():
    a.get() # 3秒后get方法不再阻塞,返回存贮的值或者异常
    print 'I live!'</p>

<p>gevent.joinall([
    gevent.spawn(setter),
    gevent.spawn(waiter),
])
</pre></p>

<p>更清晰的例子:</p>

<p><pre class="sh_python">
import gevent
from gevent.event import AsyncResult
a = AsyncResult()</p>

<p>def setter():
    gevent.sleep(3)
    a.set('Hello!')</p>

<p>def waiter():
    print a.get()</p>

<p>gevent.joinall([
    gevent.spawn(setter),
    gevent.spawn(waiter),
])
</pre>
<pre class="sh_python">
import gevent
from gevent.queue import Queue  #类似于内置的Queue</p>

<p>tasks = Queue() #队列实例</p>

<p>def worker(n):
    while not tasks.empty():
        task = tasks.get()
        print('Worker %s got task %s' % (n, task))
        gevent.sleep(0)</p>

<p>    print('Quitting time!')</p>

<p>def boss():
    for i in xrange(1,25):
        tasks.put_nowait(i) #非阻塞的把数据放到队列里面</p>

<p>gevent.spawn(boss).join()</p>

<p>gevent.joinall([
    gevent.spawn(worker, 'steve'),
    gevent.spawn(worker, 'john'),
    gevent.spawn(worker, 'nancy'),
])
</pre></p>

<p>执行结果:</p>

<p>[root@248_STAT ~]# python !$
python test.py
Worker steve got task 1 #3个用户循环的取出数据
Worker john got task 2
Worker nancy got task 3
Worker steve got task 4
Worker nancy got task 5
Worker john got task 6
Worker steve got task 7
Worker john got task 8
Worker nancy got task 9
Worker steve got task 10
Worker nancy got task 11
Worker john got task 12
Worker steve got task 13
Worker john got task 14
Worker nancy got task 15
Worker steve got task 16
Worker nancy got task 17
Worker john got task 18
Worker steve got task 19
Worker john got task 20
Worker nancy got task 21
Worker steve got task 22
Worker nancy got task 23
Worker john got task 24
Quitting time!
Quitting time!
Quitting time!</p>

<p>一个更复杂的例子:
<pre class="sh_python">
import gevent
from gevent.queue import Queue, Empty</p>

<p>tasks = Queue(maxsize=3)  #限制队列的长度</p>

<p>def worker(n):
    try:
        while True:
            task = tasks.get(timeout=1) # 减少队列,超时为1秒
            print('Worker %s got task %s' % (n, task))
            gevent.sleep(0)
    except Empty:
        print('Quitting time!')</p>

<p>def boss():
    """
    Boss will wait to hand out work until a individual worker is
    free since the maxsize of the task queue is 3.
    """</p>

<p>    for i in xrange(1,10):
        tasks.put(i)  #这里boss没有盲目的不停放入数据,而是在当最大三个队列数有空余才放入数据,事实上方法转换过程中,boss放入三个数据,worker取出三个数据,boss再放入数据....
    print('Assigned all work in iteration 1')</p>

<p>    for i in xrange(10,20):
        tasks.put(i)
    print('Assigned all work in iteration 2')</p>

<p>gevent.joinall([
    gevent.spawn(boss),
    gevent.spawn(worker, 'steve'),
    gevent.spawn(worker, 'john'),
    gevent.spawn(worker, 'bob'),
])
</pre>
<pre class="sh_python">
import gevent
from gevent.pool import Group 
def talk(msg):
    for i in xrange(3):
        print(msg)</p>

<p>g1 = gevent.spawn(talk, 'bar')
g2 = gevent.spawn(talk, 'foo')
g3 = gevent.spawn(talk, 'fizz')</p>

<p>group = Group() #保持greenlet实例的组运行,连接到没个项目,在其完成后删除
group.add(g1)
group.add(g2)
group.join()</p>

<p>group.add(g3)
group.join()
</pre></p>

<p>看更加明确的例子:
<pre class="sh_python">
import gevent
from gevent import getcurrent
from gevent.pool import Group</p>

<p>group = Group()</p>

<p>def hello_from(n):
    print('Size of group', len(group))
    print('Hello from Greenlet %s' % id(getcurrent()))  #获取当前gevent实例的id</p>

<p>group.map(hello_from, xrange(3)) #map迭代方法,参数为方法和其参数</p>

<p>def intensive(n):
    gevent.sleep(3 - n)
    return 'task', n</p>

<p>print('Ordered')</p>

<p>ogroup = Group()
for i in ogroup.imap(intensive, xrange(3)):  #相当于 itertools.imap,返回一个迭代器, 它是调用了一个其值在输入迭代器上的函数, 返回结果. 它类似于函数 <tt>map()</tt> , 只是前者在
#任意输入迭代器结束后就停止(而不是插入None值来补全所有的输入)
    print(i)</p>

<p>print('Unordered')</p>

<p>igroup = Group()
for i in igroup.imap_unordered(intensive, xrange(3)):
    print(i)
</pre></p>

<p>执行结果:</p>

<p>[root@248_STAT ~]# python test.py
(&#8216;Size of group&#8217;, 3)
Hello from Greenlet 314818960
(&#8216;Size of group&#8217;, 3)
Hello from Greenlet 314819280
(&#8216;Size of group&#8217;, 3)
Hello from Greenlet 314819440
Ordered
(&#8216;task&#8217;, 0)
(&#8216;task&#8217;, 1)
(&#8216;task&#8217;, 2)
Unordered
(&#8216;task&#8217;, 2)
(&#8216;task&#8217;, 1)
(&#8216;task&#8217;, 0)</p>

<p>还能限制pool池的大小
<pre class="sh_python">
import gevent
from gevent import getcurrent
from gevent.pool import Pool</p>

<p>pool = Pool(2)</p>

<p>def hello_from(n):
    print('Size of pool', len(pool))</p>

<p>pool.map(hello_from, xrange(3))
</pre></p>

<p>返回结果:</p>

<p>[root@248_STAT ~]# python test.py
(&#8216;Size of pool&#8217;, 2)
(&#8216;Size of pool&#8217;, 2)
(&#8216;Size of pool&#8217;, 1) #因为上面的pool容纳不了第三个,这是一个新的pool</p>

<p>以下是作者写的一个pool操作类:</p>

<p><pre class="sh_python">
from gevent.pool import Pool</p>

<p>class SocketPool(object):</p>

<p>    def __init__(self):
        self.pool = Pool(1000)  #设置池容量1000
        self.pool.start()</p>

<p>    def listen(self, socket):
        while True:
            socket.recv()</p>

<p>    def add_handler(self, socket):
        if self.pool.full(): #容量慢报错
            raise Exception("At maximum pool size")
        else: #否则执行在新的grenlet里面执行listen方法
            self.pool.spawn(self.listen, socket)</p>

<p>    def shutdown(self):
        self.pool.kill() #关闭pool
</pre></p>

<p><pre class="sh_python">
from gevent import sleep
from gevent.pool import Pool
from gevent.coros import BoundedSemaphore</p>

<p>sem = BoundedSemaphore(2) #设定对共享资源的访问数量</p>

<p>def worker1(n):
    sem.acquire() #获取资源
    print('Worker %i acquired semaphore' % n)
    sleep(0)
    sem.release()  #释放资源
    print('Worker %i released semaphore' % n)</p>

<p>def worker2(n):
    with sem: #使用with关键字
        print('Worker %i acquired semaphore' % n)
        sleep(0)
    print('Worker %i released semaphore' % n)</p>

<p>pool = Pool()
pool.map(worker1, xrange(0,2)) 
pool.map(worker2, xrange(3,6))
</pre>
执行结果:</p>

<p>[root@248_STAT ~]# python test.py
Worker 0 acquired semaphore
Worker 1 acquired semaphore  #因为pool能容纳这2个请求,所以同时获取,再释放
Worker 0 released semaphore
Worker 1 released semaphore
Worker 3 acquired semaphore #因为只能接收2个,那么5就要到下一轮
Worker 4 acquired semaphore
Worker 3 released semaphore
Worker 4 released semaphore
Worker 5 acquired semaphore
Worker 5 released semaphore</p>

<p>一个gevent教材上面说过的ping pong的那个协程例子的另一个实现:
<pre class="sh_python">
import gevent
from gevent.queue import Queue
from gevent import Greenlet</p>

<p>class Actor(gevent.Greenlet): #自定义actor类</p>

<p>    def __init__(self):
        self.inbox = Queue() #收件箱作为一个队列
        Greenlet.__init__(self) </p>

<p>    def receive(self, message): 
        raise NotImplemented() #内置常量,表面意为没有实施</p>

<p>    def _run(self): #
        self.running = True</p>

<p>        while self.running:
            message = self.inbox.get() #获取队列数据
            self.receive(message)</p>

<p>class Pinger(Actor):
    def receive(self, message): #重写方法
        print message
        pong.inbox.put('ping') #当获取收件箱有数据,获取数据,再放入数据(注意:是ping中放pong数据),其中pong是一个局部变量,它是Ponger的实例,以下的同理
        gevent.sleep(0)</p>

<p>class Ponger(Actor):
    def receive(self, message):
        print message
        ping.inbox.put('pong')
        gevent.sleep(0)</p>

<p>ping = Pinger()
pong = Ponger()</p>

<p>ping.start()
pong.start()</p>

<p>ping.inbox.put('start') #最开始都是阻塞的,给一个触发
gevent.joinall([ping, pong])
</pre></p>

<p>&nbsp;</p>

<p><code data-result="[object Object]"></code></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/archives/guanyugeventdeyixielijieer/">关于gevent的一些理解(二)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-26T00:00:00+08:00" pubdate data-updated="true">Jul 26<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>3 实际应用</strong></p>

<p>1 zeromq和gevent:</p>

<p>zeromq的介绍请参看:http://www.infoq.com/cn/news/2010/09/introduction-zero-mq</p>

<p>假设你已经安装了zeromq,gevent_zeromq(https://github.com/traviscline/gevent-zeromq.git)和pyzmq</p>

<p>一个很基础的例子:</p>

<p><pre class="sh_python">
import gevent
from gevent_zeromq import zmq</p>

<p># Global Context
context = zmq.Context() #它是GreenContext的一个简写,确保greenlet化socket</p>

<p>def server():
    server_socket = context.socket(zmq.REQ) #创建一个socket,使用mq类型模式REQ/REP(请求/回复,服务器是请求),还有PUB/SUB(发布/订阅),push/pull等
    server_socket.bind("tcp://127.0.0.1:5000") #绑定socket</p>

<p>    for request in range(1,10):
        server_socket.send("Hello")
        print('Switched to Server for ', request)
        server_socket.recv()  #这里发生上下文切换</p>

<p>def client():
    client_socket = context.socket(zmq.REP)  (客户端是回复)
    client_socket.connect("tcp://127.0.0.1:5000")  #连接server的socket端口</p>

<p>    for request in range(1,10):</p>

<p>        client_socket.recv()
        print('Switched to Client for ', request)
        client_socket.send("World")</p>

<p>publisher = gevent.spawn(server)
client    = gevent.spawn(client)</p>

<p>gevent.joinall([publisher, client])
</pre></p>

<p>执行结果:</p>

<p>[root@248_STAT ~]# python test.py
(&#8216;Switched to Server for &#8216;, 1)
(&#8216;Switched to Client for &#8216;, 1)
(&#8216;Switched to Server for &#8216;, 2)
(&#8216;Switched to Client for &#8216;, 2)
(&#8216;Switched to Server for &#8216;, 3)
(&#8216;Switched to Client for &#8216;, 3)
(&#8216;Switched to Server for &#8216;, 4)
(&#8216;Switched to Client for &#8216;, 4)
(&#8216;Switched to Server for &#8216;, 5)
(&#8216;Switched to Client for &#8216;, 5)
(&#8216;Switched to Server for &#8216;, 6)
(&#8216;Switched to Client for &#8216;, 6)
(&#8216;Switched to Server for &#8216;, 7)
(&#8216;Switched to Client for &#8216;, 7)
(&#8216;Switched to Server for &#8216;, 8)
(&#8216;Switched to Client for &#8216;, 8)
(&#8216;Switched to Server for &#8216;, 9)
(&#8216;Switched to Client for &#8216;, 9)</p>

<p>&nbsp;</p>

<p>2 telnet 服务器</p>

<p><pre class="sh_python">
from gevent.server import StreamServer #StreamServer是一个通用的TCP服务器</p>

<p>def handle(socket, address):
    socket.send("Hello from a telnet!\n")
    for i in range(5):
        socket.send(str(i) + '\n') #给socket客户端发送数据
    socket.close() #关闭客户端连接</p>

<p>server = StreamServer(('127.0.0.1', 5000), handle) #当出现连接调用定义的方法handle
server.serve_forever()
</pre></p>

<p>执行结果:</p>

<p>dongwm@localhost ~ $ nc 127.0.0.1 5000
Hello from a telnet!
0
1
2
3
4
dongwm@localhost ~ $ telnet 127.0.0.1 5000
Trying 127.0.0.1&#8230;
Connected to 127.0.0.1.
Escape character is &#8216;^]&#8217;.
Hello from a telnet!
0
1
2
3
4
Connection closed by foreign host.
<strong>3 wsgi服务器</strong></p>

<p><pre class="sh_python">
from gevent.wsgi import WSGIServer</p>

<p>def application(environ, start_response):
    status = '200 OK' #页面状态指定为200 ok
    body = '&lt;p&gt;Hello World&lt;/p&gt;'</p>

<p>    headers = [
        ('Content-Type', 'text/html')
    ]</p>

<p>    start_response(status, headers)
    return [body]</p>

<p>WSGIServer(('', 8000), application).serve_forever() #启动一个占用8000端口的wsgi服务器
</pre></p>

<p>&nbsp;</p>

<p><pre class="sh_python">
from gevent.pywsgi import WSGIServer #使用pywsgi可以我们自己定义产生结果的处理引擎</p>

<p>def application(environ, start_response):
    status = '200 OK'</p>

<p>    headers = [
        ('Content-Type', 'text/html')
    ]</p>

<p>    start_response(status, headers)
    yield "&lt;p&gt;Hello" #yield出数据
    yield "World&lt;/p&gt;"</p>

<p>WSGIServer(('', 8000), application).serve_forever()
</pre></p>

<p>我们看一个用ab(Apache Benchmark)的性能测试(更多信息请查看http://nichol.as/benchmark-of-python-web-servers),我这里只</p>

<p>对比了gevent和paste的性能比(没做系统优化,只是在同样条件下看性能差距):</p>

<p>paste的wsgi程序:</p>

<p><pre class="sh_python">
from gevent.wsgi import WSGIServer</p>

<p>def application(environ, start_response):
    status = '200 OK'
    body = '&lt;p&gt;Hello World&lt;/p&gt;'</p>

<p>    headers = [
        ('Content-Type', 'text/html')
    ]</p>

<p>    start_response(status, headers)
    return [body]</p>

<p>#WSGIServer(('', 8000), application).serve_forever()
from paste import httpserver
httpserver.serve(application, '0.0.0.0', request_queue_size=500)
</pre></p>

<p>dongwm@localhost ~ $ /usr/sbin/ab2 -n 10000 -c 100 http://127.0.0.1:8000/ #gevent的性能,条件是:并发100,请求1W
This is ApacheBench, Version 2.3 &lt;$Revision: 655654 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/</p>

<p>Benchmarking 127.0.0.1 (be patient)
Completed 1000 requests
Completed 2000 requests
Completed 3000 requests
Completed 4000 requests
Completed 5000 requests
Completed 6000 requests
Completed 7000 requests
Completed 8000 requests
Completed 9000 requests
Completed 10000 requests
Finished 10000 requests</p>

<p>Server Software:
Server Hostname:        127.0.0.1
Server Port:            8000</p>

<p>Document Path:          /
Document Length:        18 bytes</p>

<p>Concurrency Level:      100
Time taken for tests:   2.805 seconds
Complete requests:      10000
Failed requests:        0
Write errors:           0
Total transferred:      1380000 bytes
HTML transferred:       180000 bytes
Requests per second:    3564.90 [#/sec] (mean)
Time per request:       28.051 [ms] (mean)
Time per request:       0.281 [ms] (mean, across all concurrent requests)
Transfer rate:          480.43 [Kbytes/sec] received</p>

<p>Connection Times (ms)
min  mean[+/-sd] median   max
Connect:        0    0   0.2      0       2
Processing:     2   28  15.1     27      69
Waiting:        1   28  15.1     27      69
Total:          2   28  15.1     27      69</p>

<p>Percentage of the requests served within a certain time (ms)
50%     27
66%     35
75%     40
80%     42
90%     48
95%     54
98%     59
99%     62
100%     69 (longest request)</p>

<p>dongwm@localhost ~ $ /usr/sbin/ab2 -n 10000 -c 100 http://127.0.0.1:8080/  #paste的性能
This is ApacheBench, Version 2.3 &lt;$Revision: 655654 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/</p>

<p>Benchmarking 127.0.0.1 (be patient)
Completed 1000 requests
Completed 2000 requests
Completed 3000 requests
Completed 4000 requests
Completed 5000 requests
Completed 6000 requests
Completed 7000 requests
Completed 8000 requests
Completed 9000 requests
Completed 10000 requests
Finished 10000 requests</p>

<p>Server Software:        PasteWSGIServer/0.5
Server Hostname:        127.0.0.1
Server Port:            8080</p>

<p>Document Path:          /
Document Length:        18 bytes</p>

<p>Concurrency Level:      100
Time taken for tests:   4.119 seconds
Complete requests:      10000
Failed requests:        0
Write errors:           0
Total transferred:      1600000 bytes
HTML transferred:       180000 bytes
Requests per second:    2427.52 [#/sec] (mean)
Time per request:       41.194 [ms] (mean)
Time per request:       0.412 [ms] (mean, across all concurrent requests)
Transfer rate:          379.30 [Kbytes/sec] received</p>

<p>Connection Times (ms)
min  mean[+/-sd] median   max
Connect:        0    0   0.2      0       2
Processing:     2   41   5.4     41     107
Waiting:        1   41   5.2     40      97
Total:          4   41   5.3     41     107</p>

<p>Percentage of the requests served within a certain time (ms)
50%     41
66%     41
75%     42
80%     43
90%     46
95%     50
98%     56
99%     59
100%    107 (longest request)</p>

<p><strong>很不好理解吧,那我把数据直接整理下:</strong></p>

<p>1 测试用时:</p>

<p>Time taken for tests:   2.805 seconds #gevent</p>

<p>Time taken for tests:   4.119 seconds #paste 花费时间更长
2 每秒请求数:</p>

<p>Requests per second:    3564.90 [#/sec] (mean) #gevent的嘛,每秒请求数大的多
Requests per second:    2427.52 [#/sec] (mean) #paste</p>

<p>3 每请求数耗时:</p>

<p>Time per request:       28.051 [ms] (mean) #gevent耗时少
Time per request:       0.281 [ms] (mean, across all concurrent requests) #gevent并发请求时耗时少
Time per request:       41.194 [ms] (mean) #paste
Time per request:       0.412 [ms] (mean, across all concurrent requests) #paste</p>

<p>4 传输效率:</p>

<p>Transfer rate:          448.26 [Kbytes/sec] received #gevent的效率更高
Transfer rate:          379.30 [Kbytes/sec] received #paste</p>

<p>5 连接消耗的时间的分解:</p>

<p>Connection Times (ms)
min  mean[+/-sd] median   max
Connect:        0    0   0.2      0       2
Processing:     2   28  15.1     27      69
Waiting:        1   28  15.1     27      69
Total:          2   28  15.1     27      69</p>

<p>Connection Times (ms) #paste
min  mean[+/-sd] median   max
Connect:        0    0   0.2      0       2
Processing:     2   41   5.4     41     107
Waiting:        1   41   5.2     40      97
Total:          4   41   5.3     41     107 #明显其中最大用时107/97都大于gevent的69ms,最小用时gevent略强</p>

<p>6 整个场景中所有请求的响应情况。在场景中每个请求都有一个响应时间</p>

<p>Percentage of the requests served within a certain time (ms) #gevent
50%     29
66%     31
75%     34
80%     34
90%     36
95%     38
98%     42
99%     44
100%     71 (longest request)</p>

<p>可以这样理解:50%用户效应小于29ms,60%用户响应小于31ms,最长的访问响应为71ms
Percentage of the requests served within a certain time (ms) #paste
50%     41
66%     41
75%     42
80%     43
90%     46
95%     50
98%     56
99%     59
100%    107 (longest request)  #很明显,无论那个区间,paste性能都略差</p>

<p><strong>4 长轮询</strong></p>

<p><pre class="sh_python">
import gevent
from gevent.queue import Queue, Empty
from gevent.pywsgi import WSGIServer
import json</p>

<p>data_source = Queue()</p>

<p>def producer():
    while True:
        data_source.put_nowait('Hello World') #往队列非阻塞的放入数据
        gevent.sleep(1)</p>

<p>def ajax_endpoint(environ, start_response):
    status = '200 OK'
    headers = [
        ('Content-Type', 'application/json') #设定<span>网络文件的类型</span>是json
    ]
    try:
        datum = data_source.get(timeout=5)
    except Empty:
        datum = [] #假如gevent.sleep的时间设置的长一些(比如5s),在不停刷新过程中会获得空列表</p>

<p>    start_response(status, headers)
    return json.dumps(datum) #返回数据,打印出来的数据是一个带引号的字符串</p>

<p>gevent.spawn(producer)</p>

<p>WSGIServer(('', 8000), ajax_endpoint).serve_forever()
</pre>
<strong>4 聊天室</strong>(源码在这里https://github.com/sdiehl/minichat.git):</p>

<p><pre class="sh_python">
from gevent import monkey
monkey.patch_all() #给模块打包
from flask import Flask, render_template, request, json #作者在这里使用了flask框架,当然你也可以用其它比如django.tornado,bottle等</p>

<p>from gevent import queue
from gevent.pywsgi import WSGIServer</p>

<p>app = Flask(__name__) 
app.debug = True</p>

<p>class Room(object):</p>

<p>    def __init__(self):
        self.users = set()
        self.messages = []</p>

<p>    def backlog(self, size=25):
        return self.messages[-size:]</p>

<p>    def subscribe(self, user):
        self.users.add(user)</p>

<p>    def add(self, message):
        for user in self.users:
            print user
            user.queue.put_nowait(message)
        self.messages.append(message)</p>

<p>class User(object):</p>

<p>    def __init__(self):
        self.queue = queue.Queue()</p>

<p>rooms = {
    'python': Room(),
    'django': Room(),
}</p>

<p>users = {}</p>

<p>@app.route('/') #flask指定url的处理使用路由的方式,访问页面地址根目录就会执行choose_name
def choose_name():
    return render_template('choose.html') #然后调用模板choose.html,这个html文件最后使用了GET方法提交了一个uid页面(/&lt;uid&gt;)</p>

<p>@app.route('/&lt;uid&gt;') #请求被转到了这里
def main(uid):
    return render_template('main.html', #调用模板提供几个room的连接
        uid=uid,
        rooms=rooms.keys() #格局选择的连接,通过GET方法转到那个相应url:/&lt;room&gt;/&lt;uid&gt;
    )</p>

<p>@app.route('/&lt;room&gt;/&lt;uid&gt;') #请求被转到了这里
def join(room, uid):
    user = users.get(uid, None)</p>

<p>    if not user:
        users[uid] = user = User()</p>

<p>    active_room = rooms[room]
    active_room.subscribe(user)
    print 'subscribe', active_room, user</p>

<p>    messages = active_room.backlog()</p>

<p>    return render_template('room.html', #room.html包含一个POST提交方式,把你的聊天数据提交,并且更新页面(通过jquery的ajax调用url/poll/&lt;uid&gt;)
        room=room, uid=uid, messages=messages)</p>

<p>@app.route("/put/&lt;room&gt;/&lt;uid&gt;", methods=["POST"]) #通过这个url
def put(room, uid):
    user = users[uid]
    room = rooms[room]</p>

<p>    message = request.form['message']
    room.add(':'.join([uid, message]))</p>

<p>    return ''</p>

<p>@app.route("/poll/&lt;uid&gt;", methods=["POST"])
def poll(uid):
    try:
        msg = users[uid].queue.get(timeout=10)
    except queue.Empty:
        msg = []
    return json.dumps(msg) #返回队列中包含的聊天记录</p>

<p>if __name__ == "__main__":
    http = WSGIServer(('', 5000), app)
    http.serve_forever()
</pre></p>

<p>来一个更复杂带有前台后端的模型(例子来自http://blog.pythonisito.com/2011/07/gevent-zeromq-websockets-and-flot-ftw.html):</p>

<p>源码在:http://dl.dropbox.com/u/24086834/blog/20110723/zmq_websocket.tar.gz</p>

<p>其中需要修改graph.js第二行:</p>

<p>var ws = new WebSocket(&#8220;ws://localhost:9999/test&#8221;);</p>

<p>为:</p>

<p>var ws = new MozWebSocket(&#8220;ws://localhost:9999/test&#8221;);  #因为我的火狐用的websocket不同</p>

<p>这个demo.py,我来解析下:</p>

<p><pre class="sh_python">
import os
import time
import math
import json
import webbrowser</p>

<p>import paste.urlparser #paste是一个WSGI工具包，在WSGI的基础上包装了几层，让应用管理和实现变得方便</p>

<p>import gevent
from gevent_zeromq import zmq
from geventwebsocket.handler import WebSocketHandler #基于gevent的pywsgi的WebSocket的处理程序</p>

<p>def main(): #主方法
    context = zmq.Context()
    gevent.spawn(zmq_server, context) #上个例子使用joinall,这个例子是spawn+start,context是参数,也就是实例化的GreenContext
    ws_server = gevent.pywsgi.WSGIServer(
        ('', 9999), WebSocketApp(context),
        handler_class=WebSocketHandler)
    http_server = gevent.pywsgi.WSGIServer(
        ('', 8000),
        paste.urlparser.StaticURLParser(os.path.dirname(__file__))) # paste.urlparser用来处理url和静态文件
    http_server.start()  #启动grennlet实例
    ws_server.start()
    webbrowser.open('http://localhost:8000/graph.html') #启动浏览器看这个页面,当正常启动后js会画图
    zmq_producer(context)</p>

<p>def zmq_server(context):
    sock_incoming = context.socket(zmq.SUB)
    sock_outgoing = context.socket(zmq.PUB)
    sock_incoming.bind('tcp://*:5000') #发布绑定
    sock_outgoing.bind('inproc://queue') #订阅绑定,本地(通过内存)进程（线程间）通信传输
    sock_incoming.setsockopt(zmq.SUBSCRIBE, "") #这里表示对发布的所有信息都订阅
    while True:
        msg = sock_incoming.recv()
        sock_outgoing.send(msg)</p>

<p>class WebSocketApp(object):</p>

<p>    def __init__(self, context):
        self.context = context</p>

<p>    def __call__(self, environ, start_response): 
        ws = environ['wsgi.websocket']
        sock = self.context.socket(zmq.SUB) 
        sock.setsockopt(zmq.SUBSCRIBE, "") #订阅所有信息
        sock.connect('inproc://queue') #websocket连接到订阅的地址
        while True:
            msg = sock.recv()
            ws.send(msg)</p>

<p>def zmq_producer(context):  #发布的方法
    socket = context.socket(zmq.PUB)
    socket.connect('tcp://127.0.0.1:5000') #绑定到发布的socket</p>

<p>    while True:
        x = time.time() * 1000
        y = 2.5 * (1 + math.sin(x / 500))
        socket.send(json.dumps(dict(x=x, y=y))) #往发布socket发送数据,这样,数据会被inproc://queue订阅,而被websocket获取,根据数据展示
        gevent.sleep(0.05)</p>

<p>if __name__ == '__main__':
    main()
</pre></p>
</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/page/8/">&larr; Older</a></li>
    
    <li><a href="/blog/archives">博客文章</a></li>
    
    <li class="next"><a href="/page/6/">Newer &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section class='well'>
    <ul id='qq' class='nav'>
        <li class='nav-header'>我新建了一个QQ群</li>
        <li style="padding-left: 15px;">121435120</li>
        <li style="padding-left: 15px;">欢迎入伙</li>
    </ul>
</section>
<section class="well">
  <ul id="recent_posts" class="nav nav-list">
    <li class="nav-header">最近发布</li>
    
      <li class="post">
        <a href="/archives/wo-li-jie-de-pythonzui-jia-shi-jian/">我理解的python最佳实践</a>
      </li>
    
      <li class="post">
        <a href="/archives/pythonjin-jie-bi-du-hui-zong/">python进阶必读汇总</a>
      </li>
    
      <li class="post">
        <a href="/archives/liao-liao-pythonmian-shi-zhe-jian-shi-er/">聊聊python面试这件事儿</a>
      </li>
    
      <li class="post">
        <a href="/archives/idiomatic-python/">idiomatic python</a>
      </li>
    
      <li class="post">
        <a href="/archives/r-shang-chuan-wen-jian-fu-wu/">r - 上传文件服务</a>
      </li>
    
      <li class="post">
        <a href="/archives/dou-ban-tiao-mu-zu-zhao-pin/">[置顶]豆瓣条目组招聘-产品开发</a>
      </li>
    
      <li class="post">
        <a href="/archives/ast-xiang-lisp%5B%3F%5D-yang-zi-ding-yi-dai-ma-xing-wei/">AST - 像lisp一样自定义代码行为</a>
      </li>
    
      <li class="post">
        <a href="/archives/slack-alert-ba-ding-shi-ren-wu-de-jie-guo-fa-song-slackxiao-xi-de-wei-kuang-jia/">slack-alert - 把定时任务的结果发送slack消息的微框架</a>
      </li>
    
      <li class="post">
        <a href="/archives/slackshang-de-xiao-huang-ji-slackbot/">slack上的小黄鸡 - SlackBot</a>
      </li>
    
      <li class="post">
        <a href="/archives/python-cnshe-qu-huan-ying-da-jia-lai-gong-xian-dai-ma/">python-cn社区欢迎大家来贡献代码</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="recent_posts" class="nav nav-list">
  <li class="nav-header">个人网站</li>
    <li class="post"><a href="http://salogs.com">带我入行的boss</a></li>
    <li class="post"><a href="http://dongweiming.github.com/">小明明s Github Blog</a></li>
    <li class="post"><a href="http://youhouer.appspot.com/">Love story(GAE)</a></li>
    <li class="post"><a href="http://www.unixhot.com">unixhot运维社区</a></li>
    <li class="post"><a href="http://www.vpsee.com">Vpsee</a></li>
    <li class="post"><a href="http://dongweiming.github.io/sed_and_awk/">sed_and_awk</a></li>
    <li class="post"><a href="http://dongweiming.github.io/Expert-Python">Expert-Python</a></li>
  </ul>
</section>

<section class="well">
  <ul id="gh_repos" class="nav">
    <li class="nav-header">GitHub帐号</li>
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/dongweiming">@dongweiming</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        github.showRepos({
            user: 'dongweiming',
            count: 3,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/asides/github.js" type="text/javascript"> </script>
</section>




<section class="well">
   <ul id="gh_repos" class="nav">
    <li class="nav-header">标签Cloud</li>
  </ul>
  <div id="tag-cloud"></div>
</section>

<section class="well">
  <ul id="gh_repos" class="nav">
    <li class="nav-header">豆瓣阅读</li>
  </ul>
  <script type="text/javascript" src="http://www.douban.com/service/badge/62943420/?select=random&amp;n=10&amp;columns=2&amp;picsize=medium&amp;hidelogo=true&amp;hideself=true&amp;cat=book|music" ></script>
  <a href="https://www.douban.com/people/62943420">@小明明</a> on Douban 
</section>


<section class='well'>
<ul id='gh_repos' class='nav'>
<li class='nav-header'>文章统计</li>
<li>本站共有 271 篇文章</li>
</ul>
</section>


  
</aside>

      </div>
  </div>
  <footer role="contentinfo" class="page-footer"><hr>
<p>
  Copyright &copy; 2015 - Dongweiming -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'dongwm';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
